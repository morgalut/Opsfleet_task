{
  "question": "How do I handle retries in LangGraph nodes?",
  "mode": "online",
  "retrieved_docs": [
    "Document(content='Good exceptions to retry are all server errors (5xx) and selected client errors (4xx) such as 429 Too Many Requests.', source='https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.retry.RunnableRetry.html', metadata={'title': 'LangChain langchain_core.runnables.retry.RunnableRetry ‚Äî ü¶úüîó LangChain 0.2.17', 'score': 9.5})",
    "Document(content='Good exceptions to retry are all server errors (5xx) and selected client errors (4xx) such as 429 Too Many Requests.', source='https://api.python.langchain.com/en/latest/_modules/langchain_core/runnables/retry.html', metadata={'title': 'LangChain langchain_core.runnables.retry ‚Äî ü¶úüîó LangChain 0.2.17', 'score': 9.5})",
    "Document(content='Good exceptions to retry are all server errors (5xx) and selected client errors (4xx) such as 429 Too Many Requests.', source='https://api.python.langchain.com/en/v0.0.339/schema/langchain.schema.runnable.retry.RunnableRetry.html', metadata={'title': 'LangChain langchain.schema.runnable.retry.RunnableRetry ‚Äî ü¶úüîó LangChain 0.0.339', 'score': 9.5})",
    "Document(content='```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n    ```\\n\\n=== \"Async Invoke\"\\n\\n```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nawait my_workflow.ainvoke(Command(resume=some_resume_value), config)\\n    ```\\n\\n=== \"Stream\"\\n\\n```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nfor chunk in my_workflow.stream(Command(resume=some_resume_value), config):\\n        print(chunk)\\n    ```\\n\\n=== \"Async Stream\"\\n\\n```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nasync for chunk in my_workflow.astream(Command(resume=some_resume_value), config):\\n        print(chunk)\\n    ```\\n\\n**Resuming after an error**\\n\\nTo resume after an error, run the `entrypoint` with a `None` and the same **thread id** (config).\\n\\nThis assumes that the underlying **error** has been resolved and execution can proceed successfully.\\n\\n=== \"Invoke\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nmy_workflow.invoke(None, config)\\n    ```\\n\\n=== \"Async Invoke\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nawait my_workflow.ainvoke(None, config)\\n    ```\\n\\n=== \"Stream\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nfor chunk in my_workflow.stream(None, config):\\n        print(chunk)\\n    ```\\n\\n=== \"Async Stream\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nasync for chunk in my_workflow.astream(None, config):\\n        print(chunk)\\n    ```\\n\\n### Short-term memory\\n\\nWhen an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in [checkpoints](persistence.md#checkpoints).\\n\\nThis allows accessing the state from the previous invocation using the `previous` parameter.\\n\\nBy default, the `previous` parameter is the return value of the previous invocation.\\n\\n```python\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)\\n```\\n\\n#### `entrypoint.final`\\n\\n[`entrypoint.final`](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint.final) is a special primitive that can be returned from an entrypoint and allows **decoupling** the value that is **saved in the checkpoint** from the **return value of the entrypoint**.\\n\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is `entrypoint.final[return_type, save_type]`.\\n\\n```python\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n```\\n\\n## Task\\n\\nA **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\n- **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\n- **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](persistence.md) for more details).\\n\\n### Definition\\n\\nTasks are defined using the `@task` decorator, which wraps a regular Python function.\\n\\n```python\\nfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result\\n```\\n\\n!!! important \"Serialization\"\\n\\nThe **outputs** of tasks must be JSON-serializable to support checkpointing.\\n\\n### Execution\\n\\n**Tasks** can only be called from within an **entrypoint**, another **task**, or a [state graph node](./low_level.md#nodes).\\n\\nTasks _cannot_ be called directly from the main application code.\\n\\nWhen you call a **task**, it returns _immediately_ with a future object. A future is a placeholder for a result that will be available later.\\n\\nTo obtain the result of a **task**, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`).\\n\\n=== \"Synchronous Invocation\"\\n\\n```python\\n    @entrypoint(checkpointer=checkpointer)\\n    def my_workflow(some_input: int) -> int:\\n        future = slow_computation(some_input)\\n        return future.result()  # Wait for the result synchronously\\n    ```\\n\\n=== \"Asynchronous Invocation\"\\n\\n```python\\n    @entrypoint(checkpointer=checkpointer)\\n    async def my_workflow(some_input: int) -> int:\\n        return await slow_computation(some_input)  # Await result asynchronously\\n    ```\\n\\n## When to use a task\\n\\n**Tasks** are useful in the following scenarios:\\n\\n- **Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don\\'t need to recompute it when resuming the workflow.\\n- **Human-in-the-loop**: If you\\'re building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.\\n- **Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\n- **Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.smith.langchain.com/).\\n- **Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.\\n\\n## Serialization\\n\\nThere are two key aspects to serialization in LangGraph:\\n\\n1. `entrypoint` inputs and outputs must be JSON-serializable.\\n2. `task` outputs must be JSON-serializable.\\n\\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\n\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\n\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\n## Determinism\\n\\nTo utilize features like **human-in-the-loop**, any randomness should be encapsulated inside of **tasks**. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same _sequence of steps_, even if **task** results are non-deterministic.\\n\\nLangGraph achieves this behavior by persisting **task** and [**subgraph**](./subgraphs.md) results as they execute. A well-designed workflow ensures that resuming execution follows the _same sequence of steps_, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running **tasks** or **tasks** with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\n\\nWhile different runs of a workflow can produce different results, resuming a **specific** run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up **task** and **subgraph** results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\n## Idempotency\\n\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside **tasks** functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a **task** starts, but does not complete successfully. Then, if the workflow is resumed, the **task** will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\n## Common Pitfalls\\n\\n### Handling side effects\\n\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n\\n=== \"Incorrect\"\\n\\nIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.\\n\\n```python hl_lines=\"5 6\"\\n    @entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        # This code will be executed a second time when resuming the workflow.\\n        # Which is likely not what you want.\\n        with open(\"output.txt\", \"w\") as f:\\n            f.write(\"Side effect executed\")\\n        value = interrupt(\"question\")\\n        return value\\n    ```\\n\\n=== \"Correct\"\\n\\nIn this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.\\n\\n```python hl_lines=\"3 4\"\\n    from langgraph.func import task\\n\\n@task\\n    def write_to_file():\\n        with open(\"output.txt\", \"w\") as f:\\n            f.write(\"Side effect executed\")\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        # The side effect is now encapsulated in a task.\\n        write_to_file().result()\\n        value = interrupt(\"question\")\\n        return value\\n    ```\\n\\n### Non-deterministic control flow\\n\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\n- In a task: Get random number (5) ‚Üí interrupt ‚Üí resume ‚Üí (returns 5 again) ‚Üí ...\\n- Not in a task: Get random number (5) ‚Üí interrupt ‚Üí resume ‚Üí get new random number (7) ‚Üí ...\\n\\nThis is especially important when using **human-in-the-loop** workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it\\'s matched with the corresponding resume value. This matching is strictly **index-based**, so the order of the resume values should match the order of the interrupts.\\n\\nIf order of execution is not maintained when resuming, one `interrupt` call may be matched with the wrong `resume` value, leading to incorrect results.\\n\\nPlease read the section on [determinism](#determinism) for more details.\\n\\n=== \"Incorrect\"\\n\\nIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.\\n\\n```python hl_lines=\"6\"\\n    from langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        t0 = inputs[\"t0\"]\\n        t1 = time.time()\\n\\ndelta_t = t1 - t0\\n\\nif delta_t > 1:\\n            result = slow_task(1).result()\\n            value = interrupt(\"question\")\\n        else:\\n            result = slow_task(2).result()\\n            value = interrupt(\"question\")\\n\\nreturn {\\n            \"result\": result,\\n            \"value\": value\\n        }\\n    ```\\n\\n=== \"Correct\"\\n\\nIn this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.\\n\\n```python hl_lines=\"5 6 12\"\\n    import time\\n\\nfrom langgraph.func import task\\n\\n@task\\n    def get_time() -> float:\\n        return time.time()\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        t0 = inputs[\"t0\"]\\n        t1 = get_time().result()\\n\\ndelta_t = t1 - t0\\n\\nif delta_t > 1:\\n            result = slow_task(1).result()\\n            value = interrupt(\"question\")\\n        else:\\n            result = slow_task(2).result()\\n            value = interrupt(\"question\")\\n\\nreturn {\\n            \"result\": result,\\n            \"value\": value\\n        }\\n    ```\\n\\n---\\nconcepts/deployment_options.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Deployment Options\\n\\n## Free deployment\\n\\n[Local](../tutorials/langgraph-platform/local-server.md): Deploy for local testing and development.\\n\\n## Production deployment\\n\\nThere are 4 main options for deploying with the [LangGraph Platform](langgraph_platform.md):\\n\\n1. [Cloud SaaS](#cloud-saas)\\n\\n1. [Self-Hosted Data Plane](#self-hosted-data-plane)\\n\\n1. [Self-Hosted Control Plane](#self-hosted-control-plane)\\n\\n1. [Standalone Container](#standalone-container)\\n\\nA quick comparison:\\n\\n|                      | **Cloud SaaS** | **Self-Hosted Data Plane** | **Self-Hosted Control Plane** | **Standalone Container** |\\n|----------------------|----------------|----------------------------|-------------------------------|--------------------------|\\n| **[Control plane UI/API](../concepts/langgraph_control_plane.md)** | Yes | Yes | Yes | No |\\n| **CI/CD** | Managed internally by platform | Managed externally by you | Managed externally by you | Managed externally by you |\\n| **Data/compute residency** | LangChain\\'s cloud | Your cloud | Your cloud | Your cloud |\\n| **LangSmith compatibility** | Trace to LangSmith SaaS | Trace to LangSmith SaaS | Trace to Self-Hosted LangSmith | Optional tracing |\\n| **[Pricing](https://www.langchain.com/pricing-langgraph-platform)** | Plus | Enterprise | Enterprise | Enterprise |\\n\\n## Cloud SaaS\\n\\nThe [Cloud SaaS](./langgraph_cloud.md) deployment option is a fully managed model for deployment where we manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in our cloud. This option provides a simple way to deploy and manage your LangGraph Servers.\\n\\nConnect your GitHub repositories to the platform and deploy your LangGraph Servers from the [control plane UI](./langgraph_control_plane.md#control-plane-ui). The build process (i.e. CI/CD) is managed internally by the platform.\\n\\nFor more information, please see:\\n\\n* [Cloud SaaS Conceptual Guide](./langgraph_cloud.md)\\n* [How to deploy to Cloud SaaS](../cloud/deployment/cloud.md)\\n\\n## Self-Hosted Data Plane\\n\\n!!! info \"Important\"\\n    The Self-Hosted Data Plane deployment option requires an [Enterprise](../concepts/plans.md) plan.\\n\\nThe [Self-Hosted Data Plane](./langgraph_self_hosted_data_plane.md) deployment option is a \"hybrid\" model for deployment where we manage the [control plane](./langgraph_control_plane.md) in our cloud and you manage the [data plane](./langgraph_data_plane.md) in your cloud. This option provides a way to securely manage your data plane infrastructure, while offloading control plane management to us.\\n\\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server from the [control plane UI](./langgraph_control_plane.md#control-plane-ui).\\n\\nSupported Compute Platforms: [Kubernetes](https://kubernetes.io/), [Amazon ECS](https://aws.amazon.com/ecs/) (coming soon!)\\n\\nFor more information, please see:\\n\\n* [Self-Hosted Data Plane Conceptual Guide](./langgraph_self_hosted_data_plane.md)\\n* [How to deploy the Self-Hosted Data Plane](../cloud/deployment/self_hosted_data_plane.md)\\n\\n## Self-Hosted Control Plane\\n\\n!!! info \"Important\"\\n    The Self-Hosted Control Plane deployment option requires an [Enterprise](../concepts/plans.md) plan.\\n\\nThe [Self-Hosted Control Plane](./langgraph_self_hosted_control_plane.md) deployment option is a fully self-hosted model for deployment where you manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in your cloud. This option gives you full control and responsibility of the control plane and data plane infrastructure.\\n\\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server from the [control plane UI](./langgraph_control_plane.md#control-plane-ui).\\n\\nSupported Compute Platforms: [Kubernetes](https://kubernetes.io/)\\n\\nFor more information, please see:\\n\\n* [Self-Hosted Control Plane Conceptual Guide](./langgraph_self_hosted_control_plane.md)\\n* [How to deploy the Self-Hosted Control Plane](../cloud/deployment/self_hosted_control_plane.md)\\n\\n## Standalone Container\\n\\nThe [Standalone Container](./langgraph_standalone_container.md) deployment option is the least restrictive model for deployment. Deploy standalone instances of a LangGraph Server in your cloud, using any of the [available](./plans.md) license options.\\n\\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server using the container deployment tooling of your choice. Images can be deployed to any compute platform.\\n\\nFor more information, please see:\\n\\n* [Standalone Container Conceptual Guide](./langgraph_standalone_container.md)\\n* [How to deploy a Standalone Container](../cloud/deployment/standalone_container.md)\\n\\n## Related\\n\\nFor more information, please see:\\n\\n* [LangGraph Platform plans](./plans.md)\\n* [LangGraph Platform pricing](https://www.langchain.com/langgraph-platform-pricing)\\n\\n---\\nconcepts/sdk.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph SDK\\n\\nLangGraph Platform provides a python SDK for interacting with [LangGraph Server](./langgraph_server.md).\\n\\n!!! tip \"Python SDK reference\"\\n\\nFor detailed information about the Python SDK, see [Python SDK reference docs](../cloud/reference/sdk/python_sdk_ref.md).\\n\\n## Installation\\n\\nYou can install the LangGraph SDK using the following command:\\n\\n```bash\\npip install langgraph-sdk\\n```\\n\\n## Python sync vs. async\\n\\nThe Python SDK provides both synchronous (`get_sync_client`) and asynchronous (`get_client`) clients for interacting with LangGraph Server:\\n\\n=== \"Sync\"\\n\\n```python\\n    from langgraph_sdk import get_sync_client\\n\\nclient = get_sync_client(url=..., api_key=...)\\n    client.assistants.search()\\n    ```\\n\\n=== \"Async\"\\n\\n```python\\n    from langgraph_sdk import get_client\\n\\nclient = get_client(url=..., api_key=...)\\n    await client.assistants.search()\\n    ```\\n\\n## Learn more\\n\\n- [Python SDK Reference](../cloud/reference/sdk/python_sdk_ref.md)\\n- [LangGraph CLI API Reference](../cloud/reference/cli.md)\\n\\n---\\nconcepts/langgraph_server.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Server\\n\\n**LangGraph Server** offers an API for creating and managing agent-based applications. It is built on the concept of [assistants](assistants.md), which are agents configured for specific tasks, and includes built-in [persistence](persistence.md#memory-store) and a **task queue**. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions.\\n\\nUse LangGraph Server to create and manage [assistants](assistants.md), [threads](./persistence.md#threads), [runs](./assistants.md#execution), [cron jobs](../cloud/concepts/cron_jobs.md), [webhooks](../cloud/concepts/webhooks.md), and more.\\n\\n!!! tip \"API reference\"\\n  \\n    For detailed information on the API endpoints and data models, see [LangGraph Platform API reference docs](../cloud/reference/api/api_ref.html).\\n\\n## Application structure\\n\\nTo deploy a LangGraph Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.\\n\\nRead the [application structure](./application_structure.md) guide to learn how to structure your LangGraph application for deployment.\\n\\n## Parts of a deployment\\n\\nWhen you deploy LangGraph Server, you are deploying one or more [graphs](#graphs), a database for [persistence](persistence.md), and a task queue.\\n\\n### Graphs\\n\\nWhen you deploy a graph with LangGraph Server, you are deploying a \"blueprint\" for an [Assistant](assistants.md).\\n\\nAn [Assistant](assistants.md) is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases\\nthat can be served by the same graph.\\n\\nUpon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph\\'s default configuration settings.\\n\\n!!! note\\n\\nWe often think of a graph as implementing an [agent](agentic_concepts.md), but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple\\n    chatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use [multiple agents](./multi_agent.md) working in tandem.\\n\\n### Persistence and task queue\\n\\nLangGraph Server leverages a database for [persistence](persistence.md) and a task queue.\\n\\nCurrently, only [Postgres](https://www.postgresql.org/) is supported as a database for LangGraph Server and [Redis](https://redis.io/) as the task queue.\\n\\nIf you\\'re deploying using [LangGraph Platform](./langgraph_cloud.md), these components are managed for you. If you\\'re deploying LangGraph Server on your own infrastructure, you\\'ll need to set up and manage these components yourself.\\n\\nPlease review the [deployment options](./deployment_options.md) guide for more information on how these components are set up and managed.\\n\\n## Learn more\\n\\n* LangGraph [Application Structure](./application_structure.md) guide explains how to structure your LangGraph application for deployment.\\n* The [LangGraph Platform API Reference](../cloud/reference/api/api_ref.html) provides detailed information on the API endpoints and data models.\\n\\n---\\nconcepts/langgraph_platform.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Platform\\n\\nDevelop, deploy, scale, and manage agents with **LangGraph Platform** ‚Äî the purpose-built platform for long-running, agentic workflows.\\n\\n!!! tip \"Get started with LangGraph Platform\"\\n\\nCheck out the [LangGraph Platform quickstart](../tutorials/langgraph-platform/local-server.md) for instructions on how to use LangGraph Platform to run a LangGraph application locally.\\n\\n## Why use LangGraph Platform?\\n\\n<div align=\"center\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pfAQxBS5z88?si=XGS6Chydn6lhSO1S\" title=\"What is LangGraph Platform?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\\n\\nLangGraph Platform makes it easy to get your agent running in production ‚Äî  whether it‚Äôs built with LangGraph or another framework ‚Äî so you can focus on your app logic, not infrastructure. Deploy with one click to get a live endpoint, and use our robust APIs and built-in task queues to handle production scale.\\n\\n- **[Streaming Support](../cloud/how-tos/streaming.md)**: As agents grow more sophisticated, they often benefit from streaming both token outputs and intermediate states back to the user. Without this, users are left waiting for potentially long operations with no feedback. LangGraph Server provides multiple streaming modes optimized for various application needs.\\n\\n- **[Background Runs](../cloud/how-tos/background_run.md)**: For agents that take longer to process (e.g., hours), maintaining an open connection can be impractical. The LangGraph Server supports launching agent runs in the background and provides both polling endpoints and webhooks to monitor run status effectively.\\n \\n- **Support for long runs**: Regular server setups often encounter timeouts or disruptions when handling requests that take a long time to complete. LangGraph Server‚Äôs API provides robust support for these tasks by sending regular heartbeat signals, preventing unexpected connection closures during prolonged processes.\\n\\n- **Handling Burstiness**: Certain applications, especially those with real-time user interaction, may experience \"bursty\" request loads where numerous requests hit the server simultaneously. LangGraph Server includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads.\\n\\n- **[Double-texting](../cloud/how-tos/interrupt_concurrent.md)**: In user-driven applications, it‚Äôs common for users to send multiple messages rapidly. This ‚Äúdouble texting‚Äù can disrupt agent flows if not handled properly. LangGraph Server offers built-in strategies to address and manage such interactions.\\n\\n- **[Checkpointers and memory management](persistence.md#checkpoints)**: For agents needing persistence (e.g., conversation memory), deploying a robust storage solution can be complex. LangGraph Platform includes optimized [checkpointers](persistence.md#checkpoints) and a [memory store](persistence.md#memory-store), managing state across sessions without the need for custom solutions.\\n\\n- **[Human-in-the-loop support](../cloud/how-tos/human_in_the_loop_breakpoint.md)**: In many applications, users require a way to intervene in agent processes. LangGraph Server provides specialized endpoints for human-in-the-loop scenarios, simplifying the integration of manual oversight into agent workflows.\\n\\n- **[LangGraph Studio](./langgraph_studio.md)**: Enables visualization, interaction, and debugging of agentic systems that implement the LangGraph Server API protocol. Studio also integrates with LangSmith to enable tracing, evaluation, and prompt engineering.\\n\\n- **[Deployment](./deployment_options.md)**: There are four ways to deploy on LangGraph Platform: [Cloud SaaS](../concepts/langgraph_cloud.md), [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md), [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md), and [Standalone Container](../concepts/langgraph_standalone_container.md).\\n\\n---\\nconcepts/tracing.md\\n---\\n\\n# Tracing\\n\\nTraces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](../how-tos/enable-tracing.md). This enables you to do the following:\\n\\n- [Debug a locally running application](../cloud/how-tos/clone_traces_studio.md).\\n- [Evaluate the application performance](../agents/evals.md).\\n- [Monitor the application](https://docs.smith.langchain.com/observability/how_to_guides/dashboards).\\n\\nTo get started, sign up for a free account at [LangSmith](https://smith.langchain.com/).\\n\\n## Learn more\\n\\n- [Graph runs in LangSmith](../how-tos/run-id-langsmith.md)\\n- [LangSmith Observability quickstart](https://docs.smith.langchain.com/observability)\\n- [Trace with LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)\\n- [Tracing conceptual guide](https://docs.smith.langchain.com/observability/concepts#traces)\\n\\n---\\nconcepts/memory.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Memory', source='langgraph-llms-full.txt', metadata={'score': 0.5367255210876465, 'fallback': True, 'confidence': 0.9})",
    "Document(content='The recursion limit sets the maximum number of [super-steps](#graphs) the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `.invoke`/`.stream` via the config dictionary. Importantly, `recursion_limit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below:\\n\\n```python\\ngraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\\n```\\n\\nRead [this how-to](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/) to learn more about how the recursion limit works.\\n\\n## Visualization\\n\\nIt\\'s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](../how-tos/graph-api.md#visualize-your-graph) for more info.\\n\\n---\\nconcepts/langgraph_control_plane.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Control Plane\\n\\nThe term \"control plane\" is used broadly to refer to the control plane UI where users create and update [LangGraph Servers](./langgraph_server.md) (deployments) and the control plane APIs that support the UI experience.\\n\\nWhen a user makes an update through the control plane UI, the update is stored in the control plane state. The [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application polls for these updates by calling the control plane APIs.\\n\\n## Control Plane UI\\n\\nFrom the control plane UI, you can:\\n\\n- View a list of outstanding deployments.\\n- View details of an individual deployment.\\n- Create a new deployment.\\n- Update a deployment.\\n- Update environment variables for a deployment.\\n- View build and server logs of a deployment.\\n- View deployment metrics such as CPU and memory usage.\\n- Delete a deployment.\\n\\nThe Control Plane UI is embedded in [LangSmith](https://docs.smith.langchain.com/langgraph_cloud).\\n\\n## Control Plane API\\n\\nThis section describes the data model of the control plane API. The API is used to create, update, and delete deployments. See the [control plane API reference](../cloud/reference/api/api_ref_control_plane.md) for more details.\\n\\n### Deployment\\n\\nA deployment is an instance of a LangGraph Server. A single deployment can have many revisions.\\n\\n### Revision\\n\\nA revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update secrets for a deployment, a new revision must be created.\\n\\n## Control Plane Features\\n\\nThis section describes various features of the control plane.\\n\\n### Deployment Types\\n\\nFor simplicity, the control plane offers two deployment types with different resource allocations: `Development` and `Production`.\\n\\n| **Deployment Type** | **CPU/Memory**  | **Scaling**       | **Database**                                                                     |\\n| ------------------- | --------------- | ----------------- | -------------------------------------------------------------------------------- |\\n| Development         | 1 CPU, 1 GB RAM | Up to 1 replica   | 10 GB disk, no backups                                                           |\\n| Production          | 2 CPU, 2 GB RAM | Up to 10 replicas | Autoscaling disk, automatic backups, highly available (multi-zone configuration) |\\n\\nCPU and memory resources are per replica.\\n\\n!!! warning \"Immutable Deployment Type\"\\n\\nOnce a deployment is created, the deployment type cannot be changed.\\n\\n!!! info \"Self-Hosted Deployment\"\\nResources for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments can be fully customized. Deployment types are only applicable for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\\n\\n#### Production\\n\\n`Production` type deployments are suitable for \"production\" workloads. For example, select `Production` for customer-facing applications in the critical path.\\n\\nResources for `Production` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact support@langchain.dev to request an increase in resources.\\n\\n#### Development\\n\\n`Development` type deployments are suitable development and testing. For example, select `Development` for internal testing environments. `Development` type deployments are not suitable for \"production\" workloads.\\n\\n!!! danger \"Preemptible Compute Infrastructure\"\\n`Development` type deployments (API server, queue server, and database) are provisioned on preemptible compute infrastructure. This means the compute infrastructure **may be terminated at any time without notice**. This may result in intermittent...\\n\\n- Redis connection timeouts/errors\\n    - Postgres connection timeouts/errors\\n    - Failed or retrying background runs\\n\\nThis behavior is expected. Preemptible compute infrastructure **significantly reduces the cost to provision a `Development` type deployment**. By design, LangGraph Server is fault-tolerant. The implementation will automatically attempt to recover from Redis/Postgres connection errors and retry failed background runs.\\n\\n`Production` type deployments are provisioned on durable compute infrastructure, not preemptible compute infrastructure.\\n\\nDatabase disk size for `Development` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. For most use cases, [TTLs](../how-tos/ttl/configure_ttl.md) should be configured to manage disk usage. Contact support@langchain.dev to request an increase in resources.\\n\\n### Database Provisioning\\n\\nThe control plane and [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application coordinate to automatically create a Postgres database for each deployment. The database serves as the [persistence layer](../concepts/persistence.md) for the deployment.\\n\\nWhen implementing a LangGraph application, a [checkpointer](../concepts/persistence.md#checkpointer-libraries) does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured.\\n\\nThere is no direct access to the database. All access to the database occurs through the [LangGraph Server](../concepts/langgraph_server.md).\\n\\nThe database is never deleted until the deployment itself is deleted.\\n\\n!!! info\\nA custom Postgres instance can be configured for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.\\n\\n### Asynchronous Deployment\\n\\nInfrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.\\n\\n- When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.\\n- When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.\\n- The deployment process for each revision contains a build step, which can take up to a few minutes.\\n\\nThe control plane and [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application coordinate to achieve asynchronous deployments.\\n\\n### Monitoring\\n\\nAfter a deployment is ready, the control plane monitors the deployment and records various metrics, such as:\\n\\n- CPU and memory usage of the deployment.\\n- Number of container restarts.\\n- Number of replicas (this will increase with [autoscaling](../concepts/langgraph_data_plane.md#autoscaling)).\\n- [Postgres](../concepts/langgraph_data_plane.md#postgres) CPU, memory usage, and disk usage.\\n- [LangGraph Server queue](../concepts/langgraph_server.md#persistence-and-task-queue) pending/active run count.\\n- [LangGraph Server API](../concepts/langgraph_server.md) success response count, error response count, and latency.\\n\\nThese metrics are displayed as charts in the Control Plane UI.\\n\\n### LangSmith Integration\\n\\nA [LangSmith](https://docs.smith.langchain.com/) tracing project and LangSmith API key are automatically created for each deployment. The deployment uses the API key to automatically send traces to LangSmith.\\n\\n- The tracing project has the same name as the deployment.\\n- The API key has the description `LangGraph Platform: <deployment_name>`.\\n- The API key is never revealed and cannot be deleted manually.\\n- When creating a deployment, the `LANGCHAIN_TRACING` and `LANGSMITH_API_KEY`/`LANGCHAIN_API_KEY` environment variables do not need to be specified; they are set automatically by the control plane.\\n\\nWhen a deployment is deleted, the traces and the tracing project are not deleted. However, the API will be deleted when the deployment is deleted.\\n\\n---\\nconcepts/durable_execution.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Durable Execution\\n\\n**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](./human_in_the_loop.md), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).\\n\\nLangGraph\\'s built-in [persistence](./persistence.md) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](./human_in_the_loop.md) interactions -- it can be resumed from its last recorded state.\\n\\n!!! tip\\n\\nIf you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\\n    To make the most of durable execution, ensure that your workflow is designed to be [deterministic](#determinism-and-consistent-replay) and [idempotent](#determinism-and-consistent-replay) and wrap any side effects or non-deterministic operations inside [tasks](./functional_api.md#task). You can use [tasks](./functional_api.md#task) from both the [StateGraph (Graph API)](./low_level.md) and the [Functional API](./functional_api.md).\\n\\n## Requirements\\n\\nTo leverage durable execution in LangGraph, you need to:\\n\\n1. Enable [persistence](./persistence.md) in your workflow by specifying a [checkpointer](./persistence.md#checkpointer-libraries) that will save workflow progress.\\n2. Specify a [thread identifier](./persistence.md#threads) when executing a workflow. This will track the execution history for a particular instance of the workflow.\\n\\n3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside [tasks](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.task) to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see [Determinism and Consistent Replay](#determinism-and-consistent-replay).\\n\\n## Determinism and Consistent Replay\\n\\nWhen you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate [starting point](#starting-points-for-resuming-workflows) from which to pick up where it left off. This means that the workflow will replay all steps from the [starting point](#starting-points-for-resuming-workflows) until it reaches the point where it was stopped.\\n\\nAs a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside [tasks](./functional_api.md#task) or [nodes](./low_level.md#nodes).\\n\\nTo ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:\\n\\n- **Avoid Repeating Work**: If a [node](./low_level.md#nodes) contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.\\n- **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.\\n- **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow\\'s resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.\\n\\nFor some examples of pitfalls to avoid, see the [Common Pitfalls](./functional_api.md#common-pitfalls) section in the functional API, which shows\\nhow to structure your code using **tasks** to avoid these issues. The same principles apply to the [StateGraph (Graph API)](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph).\\n\\n## Durability modes\\n\\nLangGraph supports three durability modes that allow you to balance performance and data consistency based on your application\\'s requirements. The durability modes, from least to most durable, are as follows:\\n\\n- [`\"exit\"`](#exit)\\n- [`\"async\"`](#async)\\n- [`\"sync\"`](#sync)\\n\\nA higher durability mode add more overhead to the workflow execution.\\n\\n!!! version-added \"Added in version 0.6.0\"\\n\\nUse the `durability` parameter instead of `checkpoint_during` (deprecated in v0.6.0) for persistence policy management:\\n    \\n    * `durability=\"async\"` replaces `checkpoint_during=True`\\n    * `durability=\"exit\"` replaces `checkpoint_during=False`\\n    \\n    for persistence policy management, with the following mapping:\\n\\n* `checkpoint_during=True` -> `durability=\"async\"`\\n    * `checkpoint_during=False` -> `durability=\"exit\"`\\n\\n### `\"exit\"`\\n\\nChanges are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.\\n\\n### `\"async\"`\\n\\nChanges are persisted asynchronously while the next step executes. This provides good performance and durability, but there\\'s a small risk that checkpoints might not be written if the process crashes during execution.\\n\\n### `\"sync\"`\\n\\nChanges are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\\n\\nYou can specify the durability mode when calling any graph execution method:\\n\\n```python\\ngraph.stream(\\n    {\"input\": \"test\"}, \\n    durability=\"sync\"\\n)\\n```\\n\\n## Using tasks in nodes\\n\\nIf a [node](./low_level.md#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.\\n\\n=== \"Original\"\\n\\n```python hl_lines=\"16\"\\n    from typing import NotRequired\\n    from typing_extensions import TypedDict\\n    import uuid\\n\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n    from langgraph.graph import StateGraph, START, END\\n    import requests\\n\\n# Define a TypedDict to represent the state\\n    class State(TypedDict):\\n        url: str\\n        result: NotRequired[str]\\n\\ndef call_api(state: State):\\n        \"\"\"Example node that makes an API request.\"\"\"\\n        result = requests.get(state[\\'url\\']).text[:100]  # Side-effect\\n        return {\\n            \"result\": result\\n        }\\n\\n# Create a StateGraph builder and add a node for the call_api function\\n    builder = StateGraph(State)\\n    builder.add_node(\"call_api\", call_api)\\n\\n# Connect the start and end nodes to the call_api node\\n    builder.add_edge(START, \"call_api\")\\n    builder.add_edge(\"call_api\", END)\\n\\n# Specify a checkpointer\\n    checkpointer = InMemorySaver()\\n\\n# Compile the graph with the checkpointer\\n    graph = builder.compile(checkpointer=checkpointer)\\n\\n# Define a config with a thread ID.\\n    thread_id = uuid.uuid4()\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n# Invoke the graph\\n    graph.invoke({\"url\": \"https://www.example.com\"}, config)\\n    ```\\n\\n=== \"With task\"\\n\\n```python hl_lines=\"19 23\"\\n    from typing import NotRequired\\n    from typing_extensions import TypedDict\\n    import uuid\\n\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n    from langgraph.func import task\\n    from langgraph.graph import StateGraph, START, END\\n    import requests\\n\\n# Define a TypedDict to represent the state\\n    class State(TypedDict):\\n        urls: list[str]\\n        result: NotRequired[list[str]]\\n\\n@task\\n    def _make_request(url: str):\\n        \"\"\"Make a request.\"\"\"\\n        return requests.get(url).text[:100]\\n\\ndef call_api(state: State):\\n        \"\"\"Example node that makes an API request.\"\"\"\\n        requests = [_make_request(url) for url in state[\\'urls\\']]\\n        results = [request.result() for request in requests]\\n        return {\\n            \"results\": results\\n        }\\n\\n# Create a StateGraph builder and add a node for the call_api function\\n    builder = StateGraph(State)\\n    builder.add_node(\"call_api\", call_api)\\n\\n# Connect the start and end nodes to the call_api node\\n    builder.add_edge(START, \"call_api\")\\n    builder.add_edge(\"call_api\", END)\\n\\n# Specify a checkpointer\\n    checkpointer = InMemorySaver()\\n\\n# Compile the graph with the checkpointer\\n    graph = builder.compile(checkpointer=checkpointer)\\n\\n# Define a config with a thread ID.\\n    thread_id = uuid.uuid4()\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n# Invoke the graph\\n    graph.invoke({\"urls\": [\"https://www.example.com\"]}, config)\\n    ```\\n\\n## Resuming Workflows\\n\\nOnce you have enabled durable execution in your workflow, you can resume execution for the following scenarios:\\n\\n- **Pausing and Resuming Workflows:** Use the [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Interrupt) function to pause a workflow at specific points and the [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) primitive to resume it with updated state. See [**Human-in-the-Loop**](./human_in_the_loop.md) for more details.\\n- **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `None` as the input value (see this [example](../how-tos/use-functional-api.md#resuming-after-an-error) with the functional API).\\n\\n## Starting Points for Resuming Workflows\\n\\n- If you\\'re using a [StateGraph (Graph API)](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph), the starting point is the beginning of the [**node**](./low_level.md#nodes) where execution stopped.\\n- If you\\'re making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.\\n  Inside the subgraph, the starting point will be the specific [**node**](./low_level.md#nodes) where execution stopped.\\n- If you\\'re using the Functional API, the starting point is the beginning of the [**entrypoint**](./functional_api.md#entrypoint) where execution stopped.\\n\\n---\\nconcepts/langgraph_data_plane.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Data Plane\\n\\nThe term \"data plane\" is used broadly to refer to [LangGraph Servers](./langgraph_server.md) (deployments), the corresponding infrastructure for each server, and the \"listener\" application that continuously polls for updates from the [LangGraph Control Plane](./langgraph_control_plane.md).\\n\\n## Server Infrastructure\\n\\nIn addition to the [LangGraph Server](./langgraph_server.md) itself, the following infrastructure components for each server are also included in the broad definition of \"data plane\":\\n\\n- Postgres\\n- Redis\\n- Secrets store\\n- Autoscalers\\n\\n## \"Listener\" Application\\n\\nThe data plane \"listener\" application periodically calls [control plane APIs](../concepts/langgraph_control_plane.md#control-plane-api) to:\\n\\n- Determine if new deployments should be created.\\n- Determine if existing deployments should be updated (i.e. new revisions).\\n- Determine if existing deployments should be deleted.\\n\\nIn other words, the data plane \"listener\" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.\\n\\n## Postgres\\n\\nPostgres is the persistence layer for all user, run, and long-term memory data in a LangGraph Server. This stores both checkpoints (see more info [here](./persistence.md)), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info [here](./persistence.md#memory-store)).\\n\\n## Redis\\n\\nRedis is used in each LangGraph Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.\\n\\n### Communication\\n\\nAll runs in a LangGraph Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.\\n\\n1. A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from Postgres by the worker.\\n2. A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.\\n3. A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open `/stream` request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.\\n\\n### Ephemeral metadata\\n\\nRuns in a LangGraph Server may be retried for specific failures (currently only for transient Postgres errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.\\n\\n## Data Plane Features\\n\\nThis section describes various features of the data plane.\\n\\n### Data Region\\n\\n!!! info \"Only for Cloud SaaS\"\\n    Data regions are only applicable for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\\n\\nDeployments can be created in 2 data regions: US and EU\\n\\nThe data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.\\n\\n### Autoscaling\\n\\n[`Production` type](../concepts/langgraph_control_plane.md#deployment-types) deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:\\n\\n1. CPU utilization\\n1. Memory utilization\\n1. Number of pending (in progress) [runs](./assistants.md#execution)\\n\\nFor CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.\\n\\nFor number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs in 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).\\n\\nEach metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the largest number of containers.\\n\\nScale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This \"cool down\" period ensures that deployments do not scale up and down too frequently.\\n\\n### Static IP Addresses\\n\\n!!! info \"Only for Cloud SaaS\"\\nStatic IP addresses are only available for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\\n\\nAll traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:\\n\\n| US             | EU             |\\n| -------------- | -------------- |\\n| 35.197.29.146  | 34.13.192.67   |\\n| 34.145.102.123 | 34.147.105.64  |\\n| 34.169.45.153  | 34.90.22.166   |\\n| 34.82.222.17   | 34.147.36.213  |\\n| 35.227.171.135 | 34.32.137.113  |\\n| 34.169.88.30   | 34.91.238.184  |\\n| 34.19.93.202   | 35.204.101.241 |\\n| 34.19.34.50    | 35.204.48.32   |\\n\\n### Custom Postgres\\n\\n!!! info\\nCustom Postgres instances are only available for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.\\n\\nA custom Postgres instance can be used instead of the [one automatically created by the control plane](./langgraph_control_plane.md#database-provisioning). Specify the [`POSTGRES_URI_CUSTOM`](../cloud/reference/env_var.md#postgres_uri_custom) environment variable to use a custom Postgres instance.\\n\\nMultiple deployments can share the same Postgres instance. For example, for `Deployment A`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`. `<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.\\n\\n### Custom Redis\\n\\n!!! info\\nCustom Redis instances are only available for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_control_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.', source='langgraph-llms-full.txt', metadata={'score': 0.5347976684570312, 'fallback': True, 'confidence': 0.9})",
    "Document(content=\"included.\\n- [Using Webhooks with LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/how-tos/webhooks/): This documentation page provides a comprehensive guide on how to implement webhooks in the LangGraph Platform to receive updates after API calls. It includes details on supported endpoints, setup instructions for different programming languages, and examples of how to specify webhook parameters in API requests. Additionally, it covers security measures and testing tools for verifying webhook functionality.\\n- [Scheduling Tasks with Cron Jobs on LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/concepts/cron_jobs/): This page provides an overview of how to use cron jobs on the LangGraph Platform to run assistants on a defined schedule. It explains the process of setting up a cron job, including specifying the schedule, assistant, and input. Additionally, it includes links to a how-to guide and API reference for further details.\\n- [Scheduling Cron Jobs with LangGraph](https://langchain-ai.github.io/langgraph/cloud/how-tos/cron_jobs/): This page provides a comprehensive guide on how to use cron jobs with the LangGraph Platform to automate graph executions on a schedule. It includes setup instructions for various programming languages, examples of creating and deleting cron jobs, and tips for managing stateless cron jobs. Users will learn how to efficiently schedule tasks without manual intervention, ensuring timely execution of automated processes.\\n- [Adding Custom Lifespan Events in LangGraph](https://langchain-ai.github.io/langgraph/how-tos/http/custom_lifespan/): This page provides a guide on how to implement custom lifespan events in your LangGraph Platform applications, specifically for Python deployments. It covers the initialization and cleanup of resources during server startup and shutdown using FastAPI. Additionally, it includes code examples and configuration steps to help you integrate these events into your application.\\n- [Adding Custom Middleware to LangGraph Platform](https://langchain-ai.github.io/langgraph/how-tos/http/custom_middleware/): This page provides a step-by-step guide on how to add custom middleware to your server when deploying agents to the LangGraph Platform. It covers the necessary code implementation using FastAPI, configuration settings in `langgraph.json`, and instructions for testing and deploying your application. Additionally, it offers links to related topics such as custom routes and lifespan events for further customization.\\n- [Adding Custom Routes in LangGraph](https://langchain-ai.github.io/langgraph/how-tos/http/custom_routes/): This page provides a step-by-step guide on how to add custom routes to your LangGraph platform application using a Starlette or FastAPI app. It includes instructions for creating a new app, configuring the `langgraph.json` file, and testing the server locally. Additionally, it explains how custom routes can override default endpoints and offers suggestions for further customization.\\n- [LangGraph Deployment Options](https://langchain-ai.github.io/langgraph/concepts/deployment_options/): This page outlines the various deployment options available for the LangGraph Platform, including Cloud SaaS, Self-Hosted Data Plane, Self-Hosted Control Plane, and Standalone Container. Each option is described in detail, highlighting key features, management responsibilities, and compatibility. A comparison table is also provided to help users choose the best deployment strategy for their needs.\\n- [LangGraph Data Plane Overview](https://langchain-ai.github.io/langgraph/concepts/langgraph_data_plane/): This page provides a comprehensive overview of the LangGraph Data Plane, detailing its components including the server infrastructure, listener application, and data management systems like Postgres and Redis. It also covers key features such as autoscaling, static IP addresses, and custom configurations for Postgres and Redis. Additionally, the page outlines telemetry, licensing, and tracing functionalities relevant to different deployment options.\\n- [LangGraph Control Plane Overview](https://langchain-ai.github.io/langgraph/concepts/langgraph_control_plane/): This page provides a comprehensive overview of the LangGraph Control Plane, detailing its UI and API functionalities for managing LangGraph Servers. It covers deployment types, environment variables, database provisioning, and asynchronous deployment processes. Additionally, it highlights the integration with LangSmith for tracing projects.\\n- [Cloud SaaS Deployment Guide](https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/): This page provides a comprehensive guide on deploying the LangGraph Server using the Cloud SaaS model. It outlines the roles of the control plane and data plane, detailing their functionalities and management. Additionally, it includes an architectural diagram to illustrate the deployment structure.\\n- [Deployment Guide for LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/): This page provides a comprehensive guide on how to deploy applications to the LangGraph Platform using GitHub repositories. It covers prerequisites, steps for creating new deployments and revisions, managing deployment settings, and viewing logs. Additionally, it includes instructions for whitelisting IP addresses and modifying GitHub repository access.\\n- [Self-Hosted Data Plane Deployment Guide](https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_data_plane/): This page provides an overview of the Self-Hosted Data Plane deployment option, which allows users to manage their data plane infrastructure while offloading control plane management to LangChain. It outlines the requirements, architecture, and supported compute platforms for deployment. Additionally, it includes important information regarding the beta status of this deployment option.\\n- [Deploying a Self-Hosted Data Plane](https://langchain-ai.github.io/langgraph/cloud/deployment/self_hosted_data_plane/): This page provides a comprehensive guide on deploying a Self-Hosted Data Plane using Kubernetes and Amazon ECS. It outlines the prerequisites, setup steps, and configuration details necessary for a successful deployment. Additionally, it highlights the current beta status of this deployment option.\\n- [Self-Hosted Control Plane Deployment Guide](https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_control_plane/): This page provides an overview of the Self-Hosted Control Plane deployment option, currently in beta. It outlines the requirements, architecture, and compute platforms supported for deploying the control and data planes in your cloud environment. Additionally, it includes important links and resources for managing your self-hosted infrastructure.\\n- [Deploying a Self-Hosted Control Plane](https://langchain-ai.github.io/langgraph/cloud/deployment/self_hosted_control_plane/): This page provides a comprehensive guide on deploying a Self-Hosted Control Plane using Kubernetes. It outlines the prerequisites, setup steps, and configuration details necessary for a successful deployment. Additionally, it highlights the beta status of this deployment option and includes links to relevant resources for further assistance.\\n- [Deploying LangGraph Server with Standalone Container](https://langchain-ai.github.io/langgraph/concepts/langgraph_standalone_container/): This page provides a comprehensive guide on deploying a LangGraph Server using the Standalone Container option. It outlines the architecture, supported compute platforms, and Enterprise server version features. Users will find essential information on managing the data plane infrastructure without a control plane.\\n- [Deploying a Standalone Container with LangGraph](https://langchain-ai.github.io/langgraph/cloud/deployment/standalone_container/): This documentation provides a comprehensive guide on deploying a standalone container for the LangGraph application. It covers prerequisites, environment variable configurations, and deployment methods using Docker and Docker Compose. Additionally, it includes instructions for deploying on Kubernetes using Helm.\\n- [Scalability and Resilience of LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/): This page provides an overview of the scalability and resilience features of the LangGraph Platform. It details how the platform handles server and queue scalability, as well as the mechanisms in place for ensuring resilience during both graceful and hard shutdowns. Additionally, it covers the resilience strategies employed for Postgres and Redis to maintain service availability.\\n- [LangGraph Platform Plans Overview](https://langchain-ai.github.io/langgraph/concepts/plans/): This page provides an overview of the different plans available for the LangGraph Platform, including Developer, Plus, and Enterprise options. Each plan offers varying deployment options, usage limits, and features tailored to different user needs. For detailed pricing and related resources, links to additional documentation are also included.\\n\\n# Examples\\n\\n- [Building an Agentic RAG System](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/): This tutorial guides you through the process of creating a retrieval agent (RAG) system using LangChain and LangGraph. You will learn how to fetch and preprocess documents, create a retriever tool, and build an agentic RAG that intelligently decides when to retrieve information or respond directly to user queries. By the end, you'll have a functional system capable of semantic search and context-aware responses.\\n- [Building a Multi-Agent Supervisor System](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/): This tutorial guides you through the process of creating a multi-agent supervisor system using specialized agents for research and math tasks. You will learn how to set up the environment, create individual worker agents, and implement a supervisor that orchestrates their interactions. By the end, you'll have a fully functional multi-agent architecture capable of handling complex queries.\\n- [Building a SQL Agent with LangChain](https://langchain-ai.github.io/langgraph/tutorials/sql-agent/): This tutorial provides a step-by-step guide on how to create a SQL agent capable of answering questions about a SQL database. It covers the setup of necessary dependencies, configuration of a SQLite database, and the implementation of a prebuilt agent that interacts with the database to generate and execute queries. Additionally, it discusses customizing the agent for more control over its behavior.\\n- [Custom Run ID, Tags, and Metadata for LangSmith Graph Runs](https://langchain-ai.github.io/langgraph/how-tos/run-id-langsmith/): This guide provides instructions on how to pass a custom run ID and set tags and metadata for graph runs in LangSmith. It covers prerequisites, configuration options, and includes code examples for setting up and running a graph with LangGraph. Additionally, it explains how to view and filter traces in the LangSmith platform.\\n- [Custom Authentication Setup for Chatbots](https://langchain-ai.github.io/langgraph/tutorials/auth/getting_started/): This tutorial guides you through the process of setting up custom authentication for a chatbot using the LangGraph platform. You will learn how to implement token-based security to control user access, starting with a basic example and preparing for more advanced authentication methods in future tutorials. By the end, you'll have a functional chatbot that restricts access to authenticated users.\\n- [Implementing Private Conversations in Chatbots](https://langchain-ai.github.io/langgraph/tutorials/auth/resource_auth/): This tutorial guides you through extending a chatbot to enable private conversations for each user by implementing resource-level access control. You'll learn how to add authorization handlers to ensure users can only access their own threads and test the functionality to confirm proper access restrictions. Additionally, the tutorial covers scoped authorization handlers for more granular control over resource access.\\n- [Integrating OAuth2 Authentication with Supabase](https://langchain-ai.github.io/langgraph/tutorials/auth/add_auth_server/): This tutorial guides you through replacing hard-coded tokens with real user accounts using OAuth2 for secure authentication in your LangGraph application. You'll learn how to set up Supabase as your identity provider, implement token validation, and ensure proper user authorization. By the end, you'll have a production-ready authentication system that allows users to securely access their own data.\\n- [Rebuilding Graphs at Runtime in LangGraph](https://langchain-ai.github.io/langgraph/cloud/deployment/graph_rebuild/): This guide explains how to rebuild your graph at runtime with different configurations in LangGraph. It covers the necessary prerequisites, how to define graphs, and the steps to modify your graph-making function for dynamic behavior based on user input. Additionally, it provides examples of both static and dynamic graph configurations.\\n- [Interacting with RemoteGraph in LangGraph](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/): This documentation page provides a comprehensive guide on how to interact with a LangGraph Platform deployment using the RemoteGraph interface. It covers the initialization of RemoteGraph, invoking the graph both asynchronously and synchronously, and utilizing it as a subgraph. Additionally, it includes code examples in Python and JavaScript to facilitate understanding and implementation.\\n- [Deploying Agents on LangGraph Platform](https://langchain-ai.github.io/langgraph/how-tos/autogen-langgraph-platform/): This page provides a comprehensive guide on how to deploy agents like AutoGen and CrewAI using the LangGraph Platform. It covers the necessary setup, agent definition, and wrapping the agent in a LangGraph node for deployment. Additionally, it highlights the benefits of using LangGraph for scalable infrastructure and memory support.\\n- [Integrating LangGraph with React: A Comprehensive Guide](https://langchain-ai.github.io/langgraph/cloud/how-tos/use_stream_react/): This documentation provides a detailed guide on how to integrate the LangGraph platform into your React applications using the `useStream()` hook. It covers installation, key features, example implementations, and customization options for building chat experiences. Additionally, it includes advanced topics such as event handling, TypeScript support, and managing conversation threads.\\n- [Implementing Generative User Interfaces with LangGraph](https://langchain-ai.github.io/langgraph/cloud/how-tos/generative_ui_react/): This documentation provides a comprehensive guide on how to implement Generative User Interfaces (Generative UI) using the LangGraph platform. It covers prerequisites, step-by-step tutorials for defining UI components, sending them in graphs, and handling them in React applications. Additionally, it includes how-to guides for customizing components and managing UI state effectively.\\n\\n# Resources\\n\\n- [LangGraph FAQ](https://langchain-ai.github.io/langgraph/concepts/faq/): This FAQ page provides answers to common questions about LangGraph, an orchestration framework for complex agentic systems. It covers topics such as the differences between LangGraph and LangChain, performance impacts, open-source status, and compatibility with various LLMs. Additionally, it outlines the distinctions between LangGraph and LangGraph Platform, including features and deployment options.\\n- [Getting Started with LangGraph Templates](https://langchain-ai.github.io/langgraph/concepts/template_applications/): This page provides an overview of open source reference applications known as templates, designed to help users quickly build applications with LangGraph. It includes installation instructions for the LangGraph CLI, a list of available templates with their descriptions, and guidance on creating and deploying a new LangGraph app. Users can find links to repositories for each template and next steps for customizing their applications.\\n- [Guide to Using llms.txt and llms-full.txt for LLMs](https://langchain-ai.github.io/langgraph/llms-txt-overview/): This page provides an overview of the `llms.txt` and `llms-full.txt` formats, which facilitate access to programming documentation for large language models (LLMs) and agents. It outlines the differences between the two formats, usage instructions via an MCP server, and best practices for integrating these files into integrated development environments (IDEs). Additionally, it highlights considerations for managing large documentation files effectively.\\n- [Community Agents for LangGraph](https://langchain-ai.github.io/langgraph/agents/prebuilt/): This page provides a list of community-built libraries that extend the functionality of LangGraph. Each entry includes the library name, GitHub URL, a brief description, and additional metrics like weekly downloads and stars. Additionally, it outlines how to contribute your own library to the LangGraph documentation.\\n- [LangGraph Error Reference Guide](https://langchain-ai.github.io/langgraph/troubleshooting/errors/index/): This page serves as a comprehensive reference for resolving common errors encountered while using the LangGraph platform. It includes a list of error codes and links to detailed guides for troubleshooting specific issues. Users can find solutions for errors related to graph recursion, concurrent updates, node return values, and more.\\n- [Handling Recursion Limits in LangGraph](https://langchain-ai.github.io/langgraph/troubleshooting/errors/GRAPH_RECURSION_LIMIT/): This page provides guidance on managing recursion limits in LangGraph's StateGraph. It explains how to identify potential infinite loops in your graph and offers solutions for increasing the recursion limit when working with complex graphs. Additionally, it includes code examples to illustrate the concepts discussed.\\n- [Handling INVALID_CONCURRENT_GRAPH_UPDATE in LangGraph](https://langchain-ai.github.io/langgraph/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE/): This page explains the INVALID_CONCURRENT_GRAPH_UPDATE error that occurs in LangGraph when multiple nodes attempt to update the same state property concurrently. It provides an example of how this error can arise and offers a solution by using a reducer to combine values from parallel node executions. Additionally, troubleshooting tips are included to help resolve this issue.\\n- [Handling Invalid Node Return Values in LangGraph](https://langchain-ai.github.io/langgraph/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE/): This page provides guidance on the error encountered when a LangGraph node returns a non-dict value. It includes an example of incorrect node implementation and the resulting error message. Additionally, troubleshooting tips are offered to ensure that all nodes return the expected dictionary format.\\n- [Handling Multiple Subgraphs in LangGraph](https://langchain-ai.github.io/langgraph/troubleshooting/errors/MULTIPLE_SUBGRAPHS/): This page discusses the limitations of calling multiple subgraphs within a single LangGraph node when checkpointing is enabled. It provides troubleshooting tips to resolve related errors, including suggestions for compiling subgraphs without checkpointing and using the Send API for graph calls.\\n- [Handling INVALID_CHAT_HISTORY Error in create_react_agent](https://langchain-ai.github.io/langgraph/troubleshooting/errors/INVALID_CHAT_HISTORY/): This page provides an overview of the INVALID_CHAT_HISTORY error encountered in the create_react_agent function when a malformed list of messages is passed. It outlines the potential causes of the error and offers troubleshooting steps to resolve it. Users can learn how to properly invoke the graph and manage tool calls to avoid this issue.\\n- [Handling INVALID_LICENSE Error in LangGraph Platform](https://langchain-ai.github.io/langgraph/troubleshooting/errors/INVALID_LICENSE/): This page provides guidance on troubleshooting the INVALID_LICENSE error encountered when starting a self-hosted LangGraph Platform server. It outlines the scenarios in which this error may occur and offers solutions based on different deployment types. Additionally, it includes steps to verify the necessary credentials for successful deployment.\\n- [LangGraph Studio Troubleshooting Guide](https://langchain-ai.github.io/langgraph/troubleshooting/studio/): This page provides troubleshooting solutions for common connection issues encountered in LangGraph Studio, particularly with Safari and Brave browsers. It also addresses potential graph edge issues and offers methods to define routing paths for conditional edges. Users can find step-by-step instructions for resolving these issues using Cloudflare Tunnel and browser settings.\\n- [LangGraph Case Studies](https://langchain-ai.github.io/langgraph/adopters/): This page provides a comprehensive list of companies that have successfully implemented LangGraph, showcasing their unique use cases and the benefits they have achieved. Each entry includes links to detailed case studies or blog posts for further reading. If your company uses LangGraph, you are encouraged to share your success story to contribute to this growing collection.\", source='langgraph-llms.txt', metadata={'score': 0.4970594346523285, 'fallback': True, 'confidence': 0.9})",
    "Document(content='For supported providers, you can disable parallel tool calling by setting `parallel_tool_calls=False` via the `model.bind_tools()` method:\\n\\n```python hl_lines=\"3\"\\nmodel.bind_tools(\\n    tools,\\n    parallel_tool_calls=False\\n)\\n```\\n\\n??? example \"Extended example: disable parallel tool calls in a prebuilt agent\"\\n\\n```python hl_lines=\"15\"\\n    from langchain.chat_models import init_chat_model\\n\\ndef add(a: int, b: int) -> int:\\n        \"\"\"Add two numbers\"\"\"\\n        return a + b\\n\\ndef multiply(a: int, b: int) -> int:\\n        \"\"\"Multiply two numbers.\"\"\"\\n        return a * b\\n\\nmodel = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\\n    tools = [add, multiply]\\n    agent = create_react_agent(\\n        # disable parallel tool calls\\n        model=model.bind_tools(tools, parallel_tool_calls=False),\\n        tools=tools\\n    )\\n\\nagent.invoke(\\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 3 + 5 and 4 * 7?\"}]}\\n    )\\n    ```\\n\\n### Handle errors\\n\\nLangGraph provides built-in error handling for tool execution through the prebuilt [ToolNode](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode) component, used both independently and in prebuilt agents.\\n\\nBy **default**, `ToolNode` catches exceptions raised during tool execution and returns them as `ToolMessage` objects with a status indicating an error.\\n\\n```python\\nfrom langchain_core.messages import AIMessage\\nfrom langgraph.prebuilt import ToolNode\\n\\ndef multiply(a: int, b: int) -> int:\\n    if a == 42:\\n        raise ValueError(\"The ultimate error\")\\n    return a * b\\n\\n# Default error handling (enabled by default)\\ntool_node = ToolNode([multiply])\\n\\nmessage = AIMessage(\\n    content=\"\",\\n    tool_calls=[{\\n        \"name\": \"multiply\",\\n        \"args\": {\"a\": 42, \"b\": 7},\\n        \"id\": \"tool_call_id\",\\n        \"type\": \"tool_call\"\\n    }]\\n)\\n\\nresult = tool_node.invoke({\"messages\": [message]})\\n```\\n\\nOutput:\\n\\n```pycon\\n{\\'messages\\': [\\n    ToolMessage(\\n        content=\"Error: ValueError(\\'The ultimate error\\')\\\\n Please fix your mistakes.\",\\n        name=\\'multiply\\',\\n        tool_call_id=\\'tool_call_id\\',\\n        status=\\'error\\'\\n    )\\n]}\\n```\\n\\n#### Disable error handling\\n\\nTo propagate exceptions directly, disable error handling:\\n\\n```python\\ntool_node = ToolNode([multiply], handle_tool_errors=False)\\n```\\n\\nWith error handling disabled, exceptions raised by tools will propagate up, requiring explicit management.\\n\\n#### Custom error messages\\n\\nProvide a custom error message by setting the error handling parameter to a string:\\n\\n```python\\ntool_node = ToolNode(\\n    [multiply],\\n    handle_tool_errors=\"Can\\'t use 42 as the first operand, please switch operands!\"\\n)\\n```\\n\\nExample output:\\n\\n```python\\n{\\'messages\\': [\\n    ToolMessage(\\n        content=\"Can\\'t use 42 as the first operand, please switch operands!\",\\n        name=\\'multiply\\',\\n        tool_call_id=\\'tool_call_id\\',\\n        status=\\'error\\'\\n    )\\n]}\\n```\\n\\n#### Error handling in agents\\n\\nError handling in prebuilt agents (`create_react_agent`) leverages `ToolNode`:\\n\\n```python\\nfrom langgraph.prebuilt import create_react_agent\\n\\nagent = create_react_agent(\\n    model=\"anthropic:claude-3-7-sonnet-latest\",\\n    tools=[multiply]\\n)\\n\\n# Default error handling\\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]})\\n```\\n\\nTo disable or customize error handling in prebuilt agents, explicitly pass a configured `ToolNode`:\\n\\n```python\\ncustom_tool_node = ToolNode(\\n    [multiply],\\n    handle_tool_errors=\"Cannot use 42 as a first operand!\"\\n)\\n\\nagent_custom = create_react_agent(\\n    model=\"anthropic:claude-3-7-sonnet-latest\",\\n    tools=custom_tool_node\\n)\\n\\nagent_custom.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s 42 x 7?\"}]})\\n```\\n\\n### Handle large numbers of tools\\n\\nAs the number of available tools grows, you may want to limit the scope of the LLM\\'s selection, to decrease token consumption and to help manage sources of error in LLM reasoning.\\n\\nTo address this, you can dynamically adjust the tools available to a model by retrieving relevant tools at runtime using semantic search.\\n\\nSee [`langgraph-bigtool`](https://github.com/langchain-ai/langgraph-bigtool) prebuilt library for a ready-to-use implementation.\\n\\n## Prebuilt tools\\n\\n### LLM provider tools\\n\\nYou can use prebuilt tools from model providers by passing a dictionary with tool specs to the `tools` parameter of `create_react_agent`. For example, to use the `web_search_preview` tool from OpenAI:\\n\\n```python\\nfrom langgraph.prebuilt import create_react_agent\\n\\nagent = create_react_agent(\\n    model=\"openai:gpt-4o-mini\",\\n    tools=[{\"type\": \"web_search_preview\"}]\\n)\\nresponse = agent.invoke(\\n    {\"messages\": [\"What was a positive news story from today?\"]}\\n)\\n```\\n\\nPlease consult the documentation for the specific model you are using to see which tools are available and how to use them.\\n\\n### LangChain tools\\n\\nAdditionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development.\\n\\nYou can browse the full list of available integrations in the [LangChain integrations directory](https://python.langchain.com/docs/integrations/tools/).\\n\\nSome commonly used tool categories include:\\n\\n- **Search**: Bing, SerpAPI, Tavily\\n- **Code interpreters**: Python REPL, Node.js REPL\\n- **Databases**: SQL, MongoDB, Redis\\n- **Web data**: Web scraping and browsing\\n- **APIs**: OpenWeatherMap, NewsAPI, and others\\n\\nThese integrations can be configured and added to your agents using the same `tools` parameter shown in the examples above.\\n\\n---\\nhow-tos/cross-thread-persistence-functional.ipynb\\n---\\n\\n# How to add cross-thread persistence (functional API)\\n\\n!!! info \"Prerequisites\"\\n\\nThis guide assumes familiarity with the following:\\n    \\n    - <a href=\"../../concepts/functional_api/\">Functional API</a>\\n    - <a href=\"../../concepts/persistence/\">Persistence</a>\\n    - <a href=\"../../concepts/memory/\">Memory</a>\\n    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\\n\\nLangGraph allows you to persist data across **different <a href=\"../../concepts/persistence/#threads\">threads</a>**. For instance, you can store information about users (their names or preferences) in a shared (cross-thread) memory and reuse them in the new threads (e.g., new conversations).\\n\\nWhen using the <a href=\"../../concepts/functional_api/\">functional API</a>, you can set it up to store and retrieve memories by using the [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) interface:\\n\\n1. Create an instance of a `Store`\\n\\n```python\\n    from langgraph.store.memory import InMemoryStore, BaseStore\\n    \\n    store = InMemoryStore()\\n    ```\\n\\n2. Pass the `store` instance to the `entrypoint()` decorator and expose `store` parameter in the function signature:\\n\\n```python\\n    from langgraph.func import entrypoint\\n\\n@entrypoint(store=store)\\n    def workflow(inputs: dict, store: BaseStore):\\n        my_task(inputs).result()\\n        ...\\n    ```\\n    \\nIn this guide, we will show how to construct and use a workflow that has a shared memory implemented using the [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) interface.\\n\\n!!! note Note\\n\\nSupport for the [`Store`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) API that is used in this guide was added in LangGraph `v0.2.32`.\\n\\nSupport for __index__ and __query__ arguments of the [`Store`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) API that is used in this guide was added in LangGraph `v0.2.54`.\\n\\n!!! tip \"Note\"\\n\\nIf you need to add cross-thread persistence to a `StateGraph`, check out this <a href=\"../cross-thread-persistence\">how-to guide</a>.\\n\\n## Setup\\n\\nFirst, let\\'s install the required packages and set our API keys\\n\\n```shell\\npip install -U langchain_anthropic langchain_openai langgraph\\n```\\n\\n```python\\nimport getpass\\nimport os\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n_set_env(\"OPENAI_API_KEY\")\\n```\\n\\n!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started [here](https://docs.smith.langchain.com)\\n\\n## Example: simple chatbot with long-term memory\\n\\n### Define store\\n\\nIn this example we will create a workflow that will be able to retrieve information about a user\\'s preferences. We will do so by defining an `InMemoryStore` - an object that can store data in memory and query that data.\\n\\nWhen storing objects using the `Store` interface you define two things:\\n\\n* the namespace for the object, a tuple (similar to directories)\\n* the object key (similar to filenames)\\n\\nIn our example, we\\'ll be using `(\"memories\", <user_id>)` as namespace and random UUID as key for each new memory.\\n\\nImportantly, to determine the user, we will be passing `user_id` via the config keyword argument of the node function.\\n\\nLet\\'s first define our store!\\n\\n```python\\nfrom langgraph.store.memory import InMemoryStore\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nin_memory_store = InMemoryStore(\\n    index={\\n        \"embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\\n        \"dims\": 1536,\\n    }\\n)\\n```\\n\\n### Create workflow\\n\\n```python\\nimport uuid\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langchain_core.messages import BaseMessage\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.graph import add_messages\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.store.base import BaseStore\\n\\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\n@task\\ndef call_model(messages: list[BaseMessage], memory_store: BaseStore, user_id: str):\\n    namespace = (\"memories\", user_id)\\n    last_message = messages[-1]\\n    memories = memory_store.search(namespace, query=str(last_message.content))\\n    info = \"\\\\n\".join([d.value[\"data\"] for d in memories])\\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\\n\\n# Store new memories if the user asks the model to remember\\n    if \"remember\" in last_message.content.lower():\\n        memory = \"User name is Bob\"\\n        memory_store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\\n\\nresponse = model.invoke([{\"role\": \"system\", \"content\": system_msg}] + messages)\\n    return response\\n\\n# NOTE: we\\'re passing the store object here when creating a workflow via entrypoint()\\n@entrypoint(checkpointer=InMemorySaver(), store=in_memory_store)\\ndef workflow(\\n    inputs: list[BaseMessage],\\n    *,\\n    previous: list[BaseMessage],\\n    config: RunnableConfig,\\n    store: BaseStore,\\n):\\n    user_id = config[\"configurable\"][\"user_id\"]\\n    previous = previous or []\\n    inputs = add_messages(previous, inputs)\\n    response = call_model(inputs, store, user_id).result()\\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\\n```\\n\\n!!! note Note\\n\\nIf you\\'re using LangGraph Cloud or LangGraph Studio, you __don\\'t need__ to pass store to the entrypoint decorator, since it\\'s done automatically.\\n\\n### Run the workflow!\\n\\nNow let\\'s specify a user ID in the config and tell the model our name:\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\\ninput_message = {\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}\\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\\n    chunk.pretty_print()\\n```\\n```output\\n================================== Ai Message ==================================\\n\\nHello Bob! Nice to meet you. I\\'ll remember that your name is Bob. How can I help you today?\\n```\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\\n    chunk.pretty_print()\\n```\\n```output\\n================================== Ai Message ==================================\\n\\nYour name is Bob.\\n```\\nWe can now inspect our in-memory store and verify that we have in fact saved the memories for the user:\\n\\n```python\\nfor memory in in_memory_store.search((\"memories\", \"1\")):\\n    print(memory.value)\\n```\\n```output\\n{\\'data\\': \\'User name is Bob\\'}\\n```\\nLet\\'s now run the workflow for another user to verify that the memories about the first user are self contained:\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"2\"}}\\ninput_message = {\"role\": \"user\", \"content\": \"what is my name?\"}\\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\\n    chunk.pretty_print()\\n```\\n```output\\n================================== Ai Message ==================================\\n\\nI don\\'t have any information about your name. I can only see our current conversation without any prior context or personal details about you. If you\\'d like me to know your name, feel free to tell me!\\n```\\n\\n---\\nhow-tos/multi-agent-network-functional.ipynb\\n---\\n\\n# How to build a multi-agent network (functional API)\\n\\n!!! info \"Prerequisites\" \\n    This guide assumes familiarity with the following:\\n\\n- <a href=\"../../concepts/multi_agent\">Multi-agent systems</a>\\n    - <a href=\"../../concepts/functional_api\">Functional API</a>\\n    - <a href=\"../../concepts/low_level/#command\">Command</a>\\n    - <a href=\"../../concepts/low_level/\">LangGraph Glossary</a>\\n\\nIn this how-to guide we will demonstrate how to implement a <a href=\"../../concepts/multi_agent#network\">multi-agent network</a> architecture where each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. We will be using <a href=\"../../concepts/functional_api\">functional API</a> ‚Äî individual agents will be defined as tasks and the agent handoffs will be defined in the main [entrypoint()][langgraph.func.entrypoint]:\\n\\n```python\\nfrom langgraph.func import entrypoint\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_core.tools import tool\\n\\n# Define a tool to signal intent to hand off to a different agent\\n@tool(return_direct=True)\\ndef transfer_to_hotel_advisor():\\n    \"\"\"Ask hotel advisor agent for help.\"\"\"\\n    return \"Successfully transferred to hotel advisor\"\\n\\n# define an agent\\ntravel_advisor_tools = [transfer_to_hotel_advisor, ...]\\ntravel_advisor = create_react_agent(model, travel_advisor_tools)\\n\\n# define a task that calls an agent\\n@task\\ndef call_travel_advisor(messages):\\n    response = travel_advisor.invoke({\"messages\": messages})\\n    return response[\"messages\"]\\n\\n# define the multi-agent network workflow\\n@entrypoint()\\ndef workflow(messages):\\n    call_active_agent = call_travel_advisor\\n    while True:\\n        agent_messages = call_active_agent(messages).result()\\n        messages = messages + agent_messages\\n        call_active_agent = get_next_agent(messages)\\n    return messages\\n```\\n\\n## Setup\\n\\nFirst, let\\'s install the required packages\\n\\n```shell\\npip install -U langgraph langchain-anthropic\\n```\\n\\n```python\\nimport getpass\\nimport os\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n```\\n```output\\nANTHROPIC_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\n```\\n<div class=\"admonition tip\">\\n    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\\n    <p style=\"padding-top: 5px;\">\\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \\n    </p>\\n</div>\\n\\n## Travel agent example\\n\\nIn this example we will build a team of travel assistant agents that can communicate with each other.\\n\\nWe will create 2 agents:\\n\\n* `travel_advisor`: can help with travel destination recommendations. Can ask `hotel_advisor` for help.\\n* `hotel_advisor`: can help with hotel recommendations. Can ask `travel_advisor` for help.\\n\\nThis is a fully-connected network - every agent can talk to any other agent.\\n\\nFirst, let\\'s create some of the tools that the agents will be using:\\n\\n```python\\nimport random\\nfrom typing_extensions import Literal\\nfrom langchain_core.tools import tool\\n\\n@tool\\ndef get_travel_recommendations():\\n    \"\"\"Get recommendation for travel destinations\"\"\"\\n    return random.choice([\"aruba\", \"turks and caicos\"])\\n\\n@tool\\ndef get_hotel_recommendations(location: Literal[\"aruba\", \"turks and caicos\"]):\\n    \"\"\"Get hotel recommendations for a given destination.\"\"\"\\n    return {\\n        \"aruba\": [\\n            \"The Ritz-Carlton, Aruba (Palm Beach)\"\\n            \"Bucuti & Tara Beach Resort (Eagle Beach)\"\\n        ],\\n        \"turks and caicos\": [\"Grace Bay Club\", \"COMO Parrot Cay\"],\\n    }[location]\\n\\n@tool(return_direct=True)\\ndef transfer_to_hotel_advisor():\\n    \"\"\"Ask hotel advisor agent for help.\"\"\"\\n    return \"Successfully transferred to hotel advisor\"\\n\\n@tool(return_direct=True)\\ndef transfer_to_travel_advisor():\\n    \"\"\"Ask travel advisor agent for help.\"\"\"\\n    return \"Successfully transferred to travel advisor\"\\n```\\n\\n!!! note \"Transfer tools\"\\n\\nYou might have noticed that we\\'re using `@tool(return_direct=True)` in the transfer tools. This is done so that individual agents (e.g., `travel_advisor`) can exit the ReAct loop early once these tools are called. This is the desired behavior, as we want to detect when the agent calls this tool and hand control off _immediately_ to a different agent. \\n    \\n    **NOTE**: This is meant to work with the prebuilt [`create_react_agent`][langgraph.prebuilt.chat_agent_executor.create_react_agent] -- if you are building a custom agent, make sure to manually add logic for handling early exit for tools that are marked with `return_direct`.\\n\\nNow let\\'s define our agent tasks and combine them into a single multi-agent network workflow:\\n\\n```python\\nfrom langchain_core.messages import AIMessage\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langgraph.graph import add_messages\\nfrom langgraph.func import entrypoint, task\\n\\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\n# Define travel advisor ReAct agent\\ntravel_advisor_tools = [\\n    get_travel_recommendations,\\n    transfer_to_hotel_advisor,\\n]\\ntravel_advisor = create_react_agent(\\n    model,\\n    travel_advisor_tools,\\n    prompt=(\\n        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\\n        \"If you need hotel recommendations, ask \\'hotel_advisor\\' for help. \"\\n        \"You MUST include human-readable response before transferring to another agent.\"\\n    ),\\n)\\n\\n@task\\ndef call_travel_advisor(messages):\\n    # You can also add additional logic like changing the input to the agent / output from the agent, etc.\\n    # NOTE: we\\'re invoking the ReAct agent with the full history of messages in the state\\n    response = travel_advisor.invoke({\"messages\": messages})\\n    return response[\"messages\"]\\n\\n# Define hotel advisor ReAct agent\\nhotel_advisor_tools = [get_hotel_recommendations, transfer_to_travel_advisor]\\nhotel_advisor = create_react_agent(\\n    model,\\n    hotel_advisor_tools,\\n    prompt=(\\n        \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\\n        \"If you need help picking travel destinations, ask \\'travel_advisor\\' for help.\"\\n        \"You MUST include human-readable response before transferring to another agent.\"\\n    ),\\n)\\n\\n@task\\ndef call_hotel_advisor(messages):\\n    response = hotel_advisor.invoke({\"messages\": messages})\\n    return response[\"messages\"]\\n\\n@entrypoint()\\ndef workflow(messages):\\n    messages = add_messages([], messages)\\n\\ncall_active_agent = call_travel_advisor\\n    while True:\\n        agent_messages = call_active_agent(messages).result()\\n        messages = add_messages(messages, agent_messages)\\n        ai_msg = next(m for m in reversed(agent_messages) if isinstance(m, AIMessage))\\n        if not ai_msg.tool_calls:\\n            break\\n\\ntool_call = ai_msg.tool_calls[-1]\\n        if tool_call[\"name\"] == \"transfer_to_travel_advisor\":\\n            call_active_agent = call_travel_advisor\\n        elif tool_call[\"name\"] == \"transfer_to_hotel_advisor\":\\n            call_active_agent = call_hotel_advisor\\n        else:\\n            raise ValueError(f\"Expected transfer tool, got \\'{tool_call[\\'name\\']}\\'\")\\n\\nreturn messages\\n```\\n\\nLastly, let\\'s define a helper to render the agent outputs:\\n\\n```python\\nfrom langchain_core.messages import convert_to_messages\\n\\ndef pretty_print_messages(update):\\n    if isinstance(update, tuple):\\n        ns, update = update\\n        # skip parent graph updates in the printouts\\n        if len(ns) == 0:\\n            return\\n\\ngraph_id = ns[-1].split(\":\")[0]\\n        print(f\"Update from subgraph {graph_id}:\")\\n        print(\"\\\\n\")\\n\\nfor node_name, node_update in update.items():\\n        print(f\"Update from node {node_name}:\")\\n        print(\"\\\\n\")\\n\\nfor m in convert_to_messages(node_update[\"messages\"]):\\n            m.pretty_print()\\n        print(\"\\\\n\")\\n```\\n\\nLet\\'s test it out using the same input as our original multi-agent system:\\n\\n```python\\nfor chunk in workflow.stream(\\n    [\\n        {\\n            \"role\": \"user\",\\n            \"content\": \"i wanna go somewhere warm in the caribbean. pick one destination and give me hotel recommendations\",\\n        }\\n    ],\\n    subgraphs=True,\\n):\\n    pretty_print_messages(chunk)\\n```\\n```output\\nUpdate from subgraph call_travel_advisor:\\n\\nUpdate from node agent:\\n\\n================================== Ai Message ==================================\\n\\n[{\\'text\\': \"I\\'ll help you find a warm Caribbean destination and then get some hotel recommendations for you.\\\\n\\\\nLet me first get some destination recommendations for the Caribbean region.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_015vT8PkPq1VXvjrDvSpWUwJ\\', \\'input\\': {}, \\'name\\': \\'get_travel_recommendations\\', \\'type\\': \\'tool_use\\'}]\\nTool Calls:\\n  get_travel_recommendations (toolu_015vT8PkPq1VXvjrDvSpWUwJ)\\n Call ID: toolu_015vT8PkPq1VXvjrDvSpWUwJ\\n  Args:\\n\\nUpdate from subgraph call_travel_advisor:\\n\\nUpdate from node tools:\\n\\n================================= Tool Message =================================\\nName: get_travel_recommendations\\n\\nturks and caicos\\n\\nUpdate from subgraph call_travel_advisor:\\n\\nUpdate from node agent:\\n\\n================================== Ai Message ==================================\\n\\n[{\\'text\\': \"Based on the recommendation, I suggest Turks and Caicos! This beautiful British Overseas Territory is known for its stunning white-sand beaches, crystal-clear turquoise waters, and year-round warm weather. Grace Bay Beach in Providenciales is consistently ranked among the world\\'s best beaches. The islands offer excellent snorkeling, diving, and water sports opportunities, plus a relaxed Caribbean atmosphere.\\\\n\\\\nNow, let me connect you with our hotel advisor to get some specific hotel recommendations for Turks and Caicos.\", \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_01JY7pNNWFuaWoe9ymxFYiPV\\', \\'input\\': {}, \\'name\\': \\'transfer_to_hotel_advisor\\', \\'type\\': \\'tool_use\\'}]\\nTool Calls:\\n  transfer_to_hotel_advisor (toolu_01JY7pNNWFuaWoe9ymxFYiPV)\\n Call ID: toolu_01JY7pNNWFuaWoe9ymxFYiPV\\n  Args:\\n\\nUpdate from subgraph call_travel_advisor:\\n\\nUpdate from node tools:\\n\\n================================= Tool Message =================================\\nName: transfer_to_hotel_advisor\\n\\nSuccessfully transferred to hotel advisor\\n\\nUpdate from subgraph call_hotel_advisor:\\n\\nUpdate from node agent:\\n\\n================================== Ai Message ==================================\\n\\n[{\\'text\\': \\'Let me get some hotel recommendations for Turks and Caicos:\\', \\'type\\': \\'text\\'}, {\\'id\\': \\'toolu_0129ELa7jFocn16bowaGNapg\\', \\'input\\': {\\'location\\': \\'turks and caicos\\'}, \\'name\\': \\'get_hotel_recommendations\\', \\'type\\': \\'tool_use\\'}]\\nTool Calls:\\n  get_hotel_recommendations (toolu_0129ELa7jFocn16bowaGNapg)\\n Call ID: toolu_0129ELa7jFocn16bowaGNapg\\n  Args:\\n    location: turks and caicos\\n\\nUpdate from subgraph call_hotel_advisor:\\n\\nUpdate from node tools:\\n\\n================================= Tool Message =================================\\nName: get_hotel_recommendations\\n\\n[\"Grace Bay Club\", \"COMO Parrot Cay\"]\\n\\nUpdate from subgraph call_hotel_advisor:\\n\\nUpdate from node agent:\\n\\n================================== Ai Message ==================================\\n\\nHere are two excellent hotel options in Turks and Caicos:\\n\\n1. Grace Bay Club: This luxury resort is located on the world-famous Grace Bay Beach. It offers all-oceanfront suites, exceptional dining options, and personalized service. The resort features adult-only and family-friendly sections, making it perfect for any type of traveler.\\n\\n2. COMO Parrot Cay: This exclusive private island resort offers the ultimate luxury escape. It\\'s known for its pristine beach, world-class spa, and holistic wellness programs. The resort provides an intimate, secluded experience with top-notch amenities and service.\\n\\nWould you like more specific information about either of these properties or would you like to explore hotels in another destination?\\n```\\nVoila - `travel_advisor` picks a destination and then makes a decision to call `hotel_advisor` for more info!\\n\\n---\\nhow-tos/multi-agent-multi-turn-convo-functional.ipynb\\n---\\n\\n# How to add multi-turn conversation in a multi-agent application (functional API)\\n\\n!!! info \"Prerequisites\"\\n    This guide assumes familiarity with the following:\\n\\n- <a href=\"../../concepts/multi_agent\">Multi-agent systems</a>\\n    - <a href=\"../../concepts/human_in_the_loop\">Human-in-the-loop</a>\\n    - <a href=\"../../concepts/functional_api\">Functional API</a>\\n    - <a href=\"../../concepts/low_level/#command\">Command</a>\\n    - <a href=\"../../concepts/low_level/\">LangGraph Glossary</a>\\n\\nIn this how-to guide, we‚Äôll build an application that allows an end-user to engage in a *multi-turn conversation* with one or more agents. We\\'ll create a node that uses an <a href=\"../../reference/types/#langgraph.types.interrupt\">`interrupt`</a> to collect user input and routes back to the **active** agent.\\n\\nThe agents will be implemented as tasks in a workflow that executes agent steps and determines the next action:\\n\\n1. **Wait for user input** to continue the conversation, or\\n2. **Route to another agent** (or back to itself, such as in a loop) via a <a href=\"../../concepts/multi_agent/#handoffs\">**handoff**</a>.\\n\\n```python\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_core.tools import tool\\nfrom langgraph.types import interrupt\\n\\n# Define a tool to signal intent to hand off to a different agent\\n# Note: this is not using Command(goto) syntax for navigating to different agents:\\n# `workflow()` below handles the handoffs explicitly\\n@tool(return_direct=True)\\ndef transfer_to_hotel_advisor():\\n    \"\"\"Ask hotel advisor agent for help.\"\"\"\\n    return \"Successfully transferred to hotel advisor\"\\n\\n# define an agent\\ntravel_advisor_tools = [transfer_to_hotel_advisor, ...]\\ntravel_advisor = create_react_agent(model, travel_advisor_tools)\\n\\n# define a task that calls an agent\\n@task\\ndef call_travel_advisor(messages):\\n    response = travel_advisor.invoke({\"messages\": messages})\\n    return response[\"messages\"]\\n\\n# define the multi-agent network workflow\\n@entrypoint(checkpointer)\\ndef workflow(messages):\\n    call_active_agent = call_travel_advisor\\n    while True:\\n        agent_messages = call_active_agent(messages).result()\\n        ai_msg = get_last_ai_msg(agent_messages)\\n        if not ai_msg.tool_calls:\\n            user_input = interrupt(value=\"Ready for user input.\")\\n            messages = messages + [{\"role\": \"user\", \"content\": user_input}]\\n            continue', source='langgraph-llms-full.txt', metadata={'score': 0.672892689704895, 'fallback': True, 'confidence': 0.9})",
    "Document(content='```python\\n      inputs = {\\n          \"messages\": [\\n              {\\n                  \"content\": None,\\n                  \"role\": \"assistant\",\\n                  \"tool_calls\": [\\n                      {\\n                          \"id\": \"1\",\\n                          \"function\": {\\n                              \"arguments\": \\'{\"place\":\"bedroom\"}\\',\\n                              \"name\": \"get_items\",\\n                          },\\n                          \"type\": \"function\",\\n                      }\\n                  ],\\n              }\\n          ]\\n      }\\n\\nasync for chunk in graph.astream(\\n          inputs,\\n          stream_mode=\"custom\",\\n      ):\\n          print(chunk[\"content\"], end=\"|\", flush=True)\\n      ```\\n\\n### Disable streaming for specific chat models\\n\\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for\\nmodels that do not support it.\\n\\nSet `disable_streaming=True` when initializing the model.\\n\\n=== \"init_chat_model\"\\n\\n```python hl_lines=\"5\"\\n      from langchain.chat_models import init_chat_model\\n\\nmodel = init_chat_model(\\n          \"anthropic:claude-3-7-sonnet-latest\",\\n          disable_streaming=True # (1)!\\n      )\\n      ```\\n\\n1. Set `disable_streaming=True` to disable streaming for the chat model.\\n\\n=== \"chat model interface\"\\n\\n```python\\n      from langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(model=\"o1-preview\", disable_streaming=True) # (1)!\\n      ```\\n\\n1. Set `disable_streaming=True` to disable streaming for the chat model.\\n\\n### Async with Python < 3.11 { #async }\\n\\nIn Python versions < 3.11, [asyncio tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) do not support the `context` parameter.  \\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph\\'s streaming mechanisms in two key ways:\\n\\n1. You **must** explicitly pass [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated.\\n2. You **cannot** use `get_stream_writer()` in async nodes or tools ‚Äî you must pass a `writer` argument directly.\\n\\n??? example \"Extended example: async LLM call with manual config\"\\n\\n```python hl_lines=\"16 29\"\\n      from typing import TypedDict\\n      from langgraph.graph import START, StateGraph\\n      from langchain.chat_models import init_chat_model\\n\\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\\n\\nclass State(TypedDict):\\n          topic: str\\n          joke: str\\n\\nasync def call_model(state, config): # (1)!\\n          topic = state[\"topic\"]\\n          print(\"Generating joke...\")\\n          joke_response = await llm.ainvoke(\\n              [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\\n              config, # (2)!\\n          )\\n          return {\"joke\": joke_response.content}\\n\\ngraph = (\\n          StateGraph(State)\\n          .add_node(call_model)\\n          .add_edge(START, \"call_model\")\\n          .compile()\\n      )\\n\\nasync for chunk, metadata in graph.astream(\\n          {\"topic\": \"ice cream\"},\\n          stream_mode=\"messages\", # (3)!\\n      ):\\n          if chunk.content:\\n              print(chunk.content, end=\"|\", flush=True)\\n      ```\\n\\n1. Accept `config` as an argument in the async node function.\\n      2. Pass `config` to `llm.ainvoke()` to ensure proper context propagation.\\n      3. Set `stream_mode=\"messages\"` to stream LLM tokens.\\n\\n??? example \"Extended example: async custom streaming with stream writer\"\\n\\n```python hl_lines=\"8 21\"\\n      from typing import TypedDict\\n      from langgraph.types import StreamWriter\\n\\nclass State(TypedDict):\\n            topic: str\\n            joke: str\\n\\nasync def generate_joke(state: State, writer: StreamWriter): # (1)!\\n            writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\\n            return {\"joke\": f\"This is a joke about {state[\\'topic\\']}\"}\\n\\ngraph = (\\n            StateGraph(State)\\n            .add_node(generate_joke)\\n            .add_edge(START, \"generate_joke\")\\n            .compile()\\n      )\\n\\nasync for chunk in graph.astream(\\n            {\"topic\": \"ice cream\"},\\n            stream_mode=\"custom\", # (2)!\\n      ):\\n            print(chunk)\\n      ```\\n\\n1. Add `writer` as an argument in the function signature of the async node or tool. LangGraph will automatically pass the stream writer to the function.\\n      2. Set `stream_mode=\"custom\"` to receive the custom data in the stream.\\n\\n---\\nhow-tos/use-functional-api.md\\n---\\n\\n# Use the functional API\\n\\nThe [**Functional API**](../concepts/functional_api.md) allows you to add LangGraph\\'s key features ‚Äî [persistence](../concepts/persistence.md), [memory](../how-tos/memory/add-memory.md), [human-in-the-loop](../concepts/human_in_the_loop.md), and [streaming](../concepts/streaming.md) ‚Äî to your applications with minimal changes to your existing code.\\n\\n!!! tip\\n\\nFor conceptual information on the functional API, see [Functional API](../concepts/functional_api.md).\\n\\n## Creating a simple workflow\\n\\nWhen defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.\\n\\n```python\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    value = inputs[\"value\"]\\n    another_value = inputs[\"another_value\"]\\n    ...\\n\\nmy_workflow.invoke({\"value\": 1, \"another_value\": 2})\\n```\\n\\n??? example \"Extended example: simple workflow\"\\n\\n```python\\n    import uuid\\n    from langgraph.func import entrypoint, task\\n    from langgraph.checkpoint.memory import InMemorySaver\\n\\n# Task that checks if a number is even\\n    @task\\n    def is_even(number: int) -> bool:\\n        return number % 2 == 0\\n\\n# Task that formats a message\\n    @task\\n    def format_message(is_even: bool) -> str:\\n        return \"The number is even.\" if is_even else \"The number is odd.\"\\n\\n# Create a checkpointer for persistence\\n    checkpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def workflow(inputs: dict) -> str:\\n        \"\"\"Simple workflow to classify a number.\"\"\"\\n        even = is_even(inputs[\"number\"]).result()\\n        return format_message(even).result()\\n\\n# Run the workflow with a unique thread ID\\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\\n    result = workflow.invoke({\"number\": 7}, config=config)\\n    print(result)\\n    ```\\n\\n??? example \"Extended example: Compose an essay with an LLM\"\\n\\nThis example demonstrates how to use the `@task` and `@entrypoint` decorators\\n    syntactically. Given that a checkpointer is provided, the workflow results will\\n    be persisted in the checkpointer.\\n\\n```python\\n    import uuid\\n    from langchain.chat_models import init_chat_model\\n    from langgraph.func import entrypoint, task\\n    from langgraph.checkpoint.memory import InMemorySaver\\n\\nllm = init_chat_model(\\'openai:gpt-3.5-turbo\\')\\n\\n# Task: generate essay using an LLM\\n    @task\\n    def compose_essay(topic: str) -> str:\\n        \"\"\"Generate an essay about the given topic.\"\"\"\\n        return llm.invoke([\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes essays.\"},\\n            {\"role\": \"user\", \"content\": f\"Write an essay about {topic}.\"}\\n        ]).content\\n\\n# Create a checkpointer for persistence\\n    checkpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def workflow(topic: str) -> str:\\n        \"\"\"Simple workflow that generates an essay with an LLM.\"\"\"\\n        return compose_essay(topic).result()\\n\\n# Execute the workflow\\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\\n    result = workflow.invoke(\"the history of flight\", config=config)\\n    print(result)\\n    ```\\n\\n## Parallel execution\\n\\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).\\n\\n```python\\n@task\\ndef add_one(number: int) -> int:\\n    return number + 1\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef graph(numbers: list[int]) -> list[str]:\\n    futures = [add_one(i) for i in numbers]\\n    return [f.result() for f in futures]\\n```\\n\\n??? example \"Extended example: parallel LLM calls\"\\n\\nThis example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.\\n\\n```python\\n    import uuid\\n    from langchain.chat_models import init_chat_model\\n    from langgraph.func import entrypoint, task\\n    from langgraph.checkpoint.memory import InMemorySaver\\n\\n# Initialize the LLM model\\n    llm = init_chat_model(\"openai:gpt-3.5-turbo\")\\n\\n# Task that generates a paragraph about a given topic\\n    @task\\n    def generate_paragraph(topic: str) -> str:\\n        response = llm.invoke([\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\\n            {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\\n        ])\\n        return response.content\\n\\n# Create a checkpointer for persistence\\n    checkpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def workflow(topics: list[str]) -> str:\\n        \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\\n        futures = [generate_paragraph(topic) for topic in topics]\\n        paragraphs = [f.result() for f in futures]\\n        return \"\\\\n\\\\n\".join(paragraphs)\\n\\n# Run the workflow\\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\\n    result = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\\n    print(result)\\n    ```\\n\\nThis example uses LangGraph\\'s concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.\\n\\n## Calling graphs\\n\\nThe **Functional API** and the [**Graph API**](../concepts/low_level.md) can be used together in the same application as they share the same underlying runtime.\\n\\n```python\\nfrom langgraph.func import entrypoint\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph()\\n...\\nsome_graph = builder.compile()\\n\\n@entrypoint()\\ndef some_workflow(some_input: dict) -> int:\\n    # Call a graph defined using the graph API\\n    result_1 = some_graph.invoke(...)\\n    # Call another graph defined using the graph API\\n    result_2 = another_graph.invoke(...)\\n    return {\\n        \"result_1\": result_1,\\n        \"result_2\": result_2\\n    }\\n```\\n\\n??? example \"Extended example: calling a simple graph from the functional API\"\\n\\n```python\\n    import uuid\\n    from typing import TypedDict\\n    from langgraph.func import entrypoint\\n    from langgraph.checkpoint.memory import InMemorySaver\\n    from langgraph.graph import StateGraph\\n\\n# Define the shared state type\\n    class State(TypedDict):\\n        foo: int\\n\\n# Define a simple transformation node\\n    def double(state: State) -> State:\\n        return {\"foo\": state[\"foo\"] * 2}\\n\\n# Build the graph using the Graph API\\n    builder = StateGraph(State)\\n    builder.add_node(\"double\", double)\\n    builder.set_entry_point(\"double\")\\n    graph = builder.compile()\\n\\n# Define the functional API workflow\\n    checkpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def workflow(x: int) -> dict:\\n        result = graph.invoke({\"foo\": x})\\n        return {\"bar\": result[\"foo\"]}\\n\\n# Execute the workflow\\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\\n    print(workflow.invoke(5, config=config))  # Output: {\\'bar\\': 10}\\n    ```\\n\\n## Call other entrypoints\\n\\nYou can call other **entrypoints** from within an **entrypoint** or a **task**.\\n\\n```python\\n@entrypoint() # Will automatically use the checkpointer from the parent entrypoint\\ndef some_other_workflow(inputs: dict) -> int:\\n    return inputs[\"value\"]\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(inputs: dict) -> int:\\n    value = some_other_workflow.invoke({\"value\": 1})\\n    return value\\n```\\n\\n??? example \"Extended example: calling another entrypoint\"\\n\\n```python\\n    import uuid\\n    from langgraph.func import entrypoint\\n    from langgraph.checkpoint.memory import InMemorySaver\\n\\n# Initialize a checkpointer\\n    checkpointer = InMemorySaver()\\n\\n# A reusable sub-workflow that multiplies a number\\n    @entrypoint()\\n    def multiply(inputs: dict) -> int:\\n        return inputs[\"a\"] * inputs[\"b\"]\\n\\n# Main workflow that invokes the sub-workflow\\n    @entrypoint(checkpointer=checkpointer)\\n    def main(inputs: dict) -> dict:\\n        result = multiply.invoke({\"a\": inputs[\"x\"], \"b\": inputs[\"y\"]})\\n        return {\"product\": result}\\n\\n# Execute the main workflow\\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\\n    print(main.invoke({\"x\": 6, \"y\": 7}, config=config))  # Output: {\\'product\\': 42}\\n    ```\\n\\n## Streaming\\n\\nThe **Functional API** uses the same streaming mechanism as the **Graph API**. Please\\nread the [**streaming guide**](../concepts/streaming.md) section for more details.\\n\\nExample of using the streaming API to stream both updates and custom data.\\n\\n```python hl_lines=\"17\"\\nfrom langgraph.func import entrypoint\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.config import get_stream_writer # (1)!\\n\\ncheckpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef main(inputs: dict) -> int:\\n    writer = get_stream_writer() # (2)!\\n    writer(\"Started processing\") # (3)!\\n    result = inputs[\"x\"] * 2\\n    writer(f\"Result is {result}\") # (4)!\\n    return result\\n\\nconfig = {\"configurable\": {\"thread_id\": \"abc\"}}\\n\\nfor mode, chunk in main.stream( # (5)!\\n    {\"x\": 5},\\n    stream_mode=[\"custom\", \"updates\"], # (6)!\\n    config=config\\n):\\n    print(f\"{mode}: {chunk}\")\\n```\\n\\n1. Import `get_stream_writer` from `langgraph.config`.\\n2. Obtain a stream writer instance within the entrypoint.\\n3. Emit custom data before computation begins.\\n4. Emit another custom message after computing the result.\\n5. Use `.stream()` to process streamed output.\\n6. Specify which streaming modes to use.\\n\\n```pycon\\n(\\'updates\\', {\\'add_one\\': 2})\\n(\\'updates\\', {\\'add_two\\': 3})\\n(\\'custom\\', \\'hello\\')\\n(\\'custom\\', \\'world\\')\\n(\\'updates\\', {\\'main\\': 5})\\n```\\n\\n!!! important \"Async with Python < 3.11\"\\n\\nIf using Python < 3.11 and writing async code, using `get_stream_writer()` will not work. Instead please\\n    use the `StreamWriter` class directly. See [Async with Python < 3.11](../how-tos/streaming.md#async) for more details.\\n\\n```python hl_lines=\"4\"\\n    from langgraph.types import StreamWriter\\n\\n@entrypoint(checkpointer=checkpointer)\\n    async def main(inputs: dict, writer: StreamWriter) -> int:\\n        ...\\n    ```\\n\\n## Retry policy\\n\\n```python\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import RetryPolicy\\n\\n# This variable is just used for demonstration purposes to simulate a network failure.\\n# It\\'s not something you will have in your actual code.\\nattempts = 0\\n\\n# Let\\'s configure the RetryPolicy to retry on ValueError.\\n# The default RetryPolicy is optimized for retrying specific network errors.\\nretry_policy = RetryPolicy(retry_on=ValueError)\\n\\n@task(retry_policy=retry_policy)\\ndef get_info():\\n    global attempts\\n    attempts += 1\\n\\nif attempts < 2:\\n        raise ValueError(\\'Failure\\')\\n    return \"OK\"\\n\\ncheckpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef main(inputs, writer):\\n    return get_info().result()\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmain.invoke({\\'any_input\\': \\'foobar\\'}, config=config)\\n```\\n\\n```pycon\\n\\'OK\\'\\n```\\n\\n## Caching Tasks\\n\\n```python\\nimport time\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import CachePolicy\\n\\n@task(cache_policy=CachePolicy(ttl=120))  # (1)!\\ndef slow_add(x: int) -> int:\\n    time.sleep(1)\\n    return x * 2\\n\\n@entrypoint(cache=InMemoryCache())\\ndef main(inputs: dict) -> dict[str, int]:\\n    result1 = slow_add(inputs[\"x\"]).result()\\n    result2 = slow_add(inputs[\"x\"]).result()\\n    return {\"result1\": result1, \"result2\": result2}\\n\\nfor chunk in main.stream({\"x\": 5}, stream_mode=\"updates\"):\\n    print(chunk)\\n\\n#> {\\'slow_add\\': 10}\\n#> {\\'slow_add\\': 10, \\'__metadata__\\': {\\'cached\\': True}}\\n#> {\\'main\\': {\\'result1\\': 10, \\'result2\\': 10}}\\n```\\n\\n1. `ttl` is specified in seconds. The cache will be invalidated after this time.\\n\\n## Resuming after an error\\n\\n```python\\nimport time\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import StreamWriter\\n\\n# This variable is just used for demonstration purposes to simulate a network failure.\\n# It\\'s not something you will have in your actual code.\\nattempts = 0\\n\\n@task()\\ndef get_info():\\n    \"\"\"\\n    Simulates a task that fails once before succeeding.\\n    Raises an exception on the first attempt, then returns \"OK\" on subsequent tries.\\n    \"\"\"\\n    global attempts\\n    attempts += 1\\n\\nif attempts < 2:\\n        raise ValueError(\"Failure\")  # Simulate a failure on the first attempt\\n    return \"OK\"\\n\\n# Initialize an in-memory checkpointer for persistence\\ncheckpointer = InMemorySaver()\\n\\n@task\\ndef slow_task():\\n    \"\"\"\\n    Simulates a slow-running task by introducing a 1-second delay.\\n    \"\"\"\\n    time.sleep(1)\\n    return \"Ran slow task.\"\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef main(inputs, writer: StreamWriter):\\n    \"\"\"\\n    Main workflow function that runs the slow_task and get_info tasks sequentially.\\n\\nParameters:\\n    - inputs: Dictionary containing workflow input values.\\n    - writer: StreamWriter for streaming custom data.\\n\\nThe workflow first executes `slow_task` and then attempts to execute `get_info`,\\n    which will fail on the first invocation.\\n    \"\"\"\\n    slow_task_result = slow_task().result()  # Blocking call to slow_task\\n    get_info().result()  # Exception will be raised here on the first attempt\\n    return slow_task_result\\n\\n# Workflow execution configuration with a unique thread identifier\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"  # Unique identifier to track workflow execution\\n    }\\n}\\n\\n# This invocation will take ~1 second due to the slow_task execution\\ntry:\\n    # First invocation will raise an exception due to the `get_info` task failing\\n    main.invoke({\\'any_input\\': \\'foobar\\'}, config=config)\\nexcept ValueError:\\n    pass  # Handle the failure gracefully\\n```\\n\\nWhen we resume execution, we won\\'t need to re-run the `slow_task` as its result is already saved in the checkpoint.\\n\\n```python\\nmain.invoke(None, config=config)\\n```\\n\\n```pycon\\n\\'Ran slow task.\\'\\n```\\n\\n## Human-in-the-loop\\n\\nThe functional API supports [human-in-the-loop](../concepts/human_in_the_loop.md) workflows using the `interrupt` function and the `Command` primitive.\\n\\n### Basic human-in-the-loop workflow\\n\\nWe will create three [tasks](../concepts/functional_api.md#task):\\n\\n1. Append `\"bar\"`.\\n2. Pause for human input. When resuming, append human input.\\n3. Append `\"qux\"`.\\n\\n```python\\nfrom langgraph.func import entrypoint, task\\nfrom langgraph.types import Command, interrupt\\n\\n@task\\ndef step_1(input_query):\\n    \"\"\"Append bar.\"\"\"\\n    return f\"{input_query} bar\"\\n\\n@task\\ndef human_feedback(input_query):\\n    \"\"\"Append user input.\"\"\"\\n    feedback = interrupt(f\"Please provide feedback: {input_query}\")\\n    return f\"{input_query} {feedback}\"\\n\\n@task\\ndef step_3(input_query):\\n    \"\"\"Append qux.\"\"\"\\n    return f\"{input_query} qux\"\\n```\\n\\nWe can now compose these tasks in an [entrypoint](../concepts/functional_api.md#entrypoint):\\n\\n```python\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\ncheckpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef graph(input_query):\\n    result_1 = step_1(input_query).result()\\n    result_2 = human_feedback(result_1).result()\\n    result_3 = step_3(result_2).result()\\n\\nreturn result_3\\n```\\n\\n[interrupt()](../how-tos/human_in_the_loop/add-human-in-the-loop.md#pause-using-interrupt) is called inside a task, enabling a human to review and edit the output of the previous task. The results of prior tasks-- in this case `step_1`-- are persisted, so that they are not run again following the `interrupt`.\\n\\nLet\\'s send in a query string:\\n\\n```python\\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\\n\\nfor event in graph.stream(\"foo\", config):\\n    print(event)\\n    print(\"\\\\n\")\\n```\\n\\nNote that we\\'ve paused with an `interrupt` after `step_1`. The interrupt provides instructions to resume the run. To resume, we issue a [Command](../how-tos/human_in_the_loop/add-human-in-the-loop.md#resume-using-the-command-primitive) containing the data expected by the `human_feedback` task.\\n\\n```python\\n# Continue execution\\nfor event in graph.stream(Command(resume=\"baz\"), config):\\n    print(event)\\n    print(\"\\\\n\")\\n```\\n\\nAfter resuming, the run proceeds through the remaining step and terminates as expected.\\n\\n### Review tool calls\\n\\nTo review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](../how-tos/human_in_the_loop/add-human-in-the-loop.md#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.\\n\\nGiven a tool call, our function will `interrupt` for human review. At that point we can either:\\n\\n- Accept the tool call\\n- Revise the tool call and continue\\n- Generate a custom tool message (e.g., instructing the model to re-format its tool call)\\n\\n```python\\nfrom typing import Union\\n\\ndef review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:\\n    \"\"\"Review a tool call, returning a validated version.\"\"\"\\n    human_review = interrupt(\\n        {\\n            \"question\": \"Is this correct?\",\\n            \"tool_call\": tool_call,\\n        }\\n    )\\n    review_action = human_review[\"action\"]\\n    review_data = human_review.get(\"data\")\\n    if review_action == \"continue\":\\n        return tool_call\\n    elif review_action == \"update\":\\n        updated_tool_call = {**tool_call, **{\"args\": review_data}}\\n        return updated_tool_call\\n    elif review_action == \"feedback\":\\n        return ToolMessage(\\n            content=review_data, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]\\n        )\\n```\\n\\nWe can now update our [entrypoint](../concepts/functional_api.md#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the `ToolMessage` supplied by the human. The results of prior tasks ‚Äî in this case the initial model call ‚Äî are persisted, so that they are not run again following the `interrupt`.\\n\\n```python\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.graph.message import add_messages\\nfrom langgraph.types import Command, interrupt\\n\\ncheckpointer = InMemorySaver()\\n\\n@entrypoint(checkpointer=checkpointer)\\ndef agent(messages, previous):\\n    if previous is not None:\\n        messages = add_messages(previous, messages)\\n\\nllm_response = call_model(messages).result()\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n# Review tool calls\\n        tool_results = []\\n        tool_calls = []\\n        for i, tool_call in enumerate(llm_response.tool_calls):\\n            review = review_tool_call(tool_call)\\n            if isinstance(review, ToolMessage):\\n                tool_results.append(review)\\n            else:  # is a validated tool call\\n                tool_calls.append(review)\\n                if review != tool_call:\\n                    llm_response.tool_calls[i] = review  # update message\\n\\n# Execute remaining tool calls\\n        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]\\n        remaining_tool_results = [fut.result() for fut in tool_result_futures]\\n\\n# Append to message list\\n        messages = add_messages(\\n            messages,\\n            [llm_response, *tool_results, *remaining_tool_results],\\n        )\\n\\n# Call model again\\n        llm_response = call_model(messages).result()\\n\\n# Generate final response\\n    messages = add_messages(messages, llm_response)\\n    return entrypoint.final(value=llm_response, save=messages)\\n```\\n\\n## Short-term memory\\n\\nShort-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](../concepts/functional_api.md#short-term-memory) for more details.\\n\\n### Manage checkpoints\\n\\nYou can view and delete the information stored by the checkpointer.\\n\\n#### View thread state (checkpoint)\\n\\n```python hl_lines=\"3 6 10\"\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\",\\n        # optionally provide an ID for a specific checkpoint,\\n        # otherwise the latest checkpoint is shown\\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\\n\\n}\\n}\\ngraph.get_state(config)\\n```\\n\\n```\\nStateSnapshot(\\n    values={\\'messages\\': [HumanMessage(content=\"hi! I\\'m bob\"), AIMessage(content=\\'Hi Bob! How are you doing today?), HumanMessage(content=\"what\\'s my name?\"), AIMessage(content=\\'Your name is Bob.\\')]}, next=(),\\n    config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-1f5b-6704-8004-820c16b69a5a\\'}},\\n    metadata={\\n        \\'source\\': \\'loop\\',\\n        \\'writes\\': {\\'call_model\\': {\\'messages\\': AIMessage(content=\\'Your name is Bob.\\')}},\\n        \\'step\\': 4,\\n        \\'parents\\': {},\\n        \\'thread_id\\': \\'1\\'\\n    },\\n    created_at=\\'2025-05-05T16:01:24.680462+00:00\\',\\n    parent_config={\\'configurable\\': {\\'thread_id\\': \\'1\\', \\'checkpoint_ns\\': \\'\\', \\'checkpoint_id\\': \\'1f029ca3-1790-6b0a-8003-baf965b6a38f\\'}},\\n    tasks=(),\\n    interrupts=()\\n)\\n```\\n\\n#### View the history of the thread (checkpoints)\\n\\n```python hl_lines=\"3 6\"\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\nlist(graph.get_state_history(config))\\n```', source='langgraph-llms-full.txt', metadata={'score': 0.6598389148712158, 'fallback': True, 'confidence': 0.9})",
    "Document(content='All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the LangGraph Server unavailable.\\n\\n---\\nconcepts/low_level.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Graph API concepts\\n\\n## Graphs\\n\\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\\n\\n1. [`State`](#state): A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n2. [`Nodes`](#nodes): Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n3. [`Edges`](#edges): Functions that determine which `Node` to execute next based on the current state. They can be conditional branches or fixed transitions.\\n\\nBy composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: `Nodes` and `Edges` are nothing more than functions - they can contain an LLM or just good ol\\' code.\\n\\nIn short: _nodes do the work, edges tell what to do next_.\\n\\nLangGraph\\'s underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google\\'s [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete \"super-steps.\"\\n\\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or \"channels\"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit.\\n\\n### StateGraph\\n\\nThe `StateGraph` class is the main graph class to use. This is parameterized by a user defined `State` object.\\n\\n### Compiling your graph\\n\\nTo build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed?\\n\\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](./persistence.md) and breakpoints. You compile your graph by just calling the `.compile` method:\\n\\n```python\\ngraph = graph_builder.compile(...)\\n```\\n\\nYou **MUST** compile your graph before you can use it.\\n\\n## State\\n\\nThe first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.\\n\\n### Schema\\n\\nThe main documented way to specify the schema of a graph is by using a [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict). If you want to provide default values in your state, use a [`dataclass`](https://docs.python.org/3/library/dataclasses.html). We also support using a Pydantic [BaseModel](../how-tos/graph-api.md#use-pydantic-models-for-graph-state) as your graph state if you want recursive data validation (though note that pydantic is less performant than a `TypedDict` or `dataclass`).\\n\\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the [guide here](../how-tos/graph-api.md#define-input-and-output-schemas) for how to use.\\n\\n#### Multiple schemas\\n\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\\n\\n- Internal nodes can pass information that is not required in the graph\\'s input / output.\\n- We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`.\\n\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an \"internal\" schema that contains _all_ keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the \"internal\" schema to constrain the input and output of the graph. See [this guide](../how-tos/graph-api.md#define-input-and-output-schemas) for more detail.\\n\\nLet\\'s look at an example:\\n\\n```python\\nclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n```\\n\\nThere are two subtle and important points to note here:\\n\\n1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node _can write to any state channel in the graph state._ The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.\\n\\n2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because _nodes can also declare additional state channels_ as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.\\n\\n### Reducers\\n\\nReducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\n#### Default Reducer\\n\\nThese two examples show how to use the default reducer:\\n\\n**Example A:**\\n\\n```python\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n```\\n\\nIn this example, no reducer functions are specified for any key. Let\\'s assume the input to the graph is:\\n\\n`{\"foo\": 1, \"bar\": [\"hi\"]}`. Let\\'s then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"bye\"]}`\\n\\n**Example B:**\\n\\n```python\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]\\n```\\n\\nIn this example, we\\'ve used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let\\'s assume the input to the graph is `{\"foo\": 1, \"bar\": [\"hi\"]}`. Let\\'s then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}`. Notice here that the `bar` key is updated by adding the two lists together.\\n\\n### Working with Messages in Graph State\\n\\n#### Why use messages?\\n\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain\\'s [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as `HumanMessage` (user input) or `AIMessage` (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide.\\n\\n#### Using Messages in your Graph\\n\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don\\'t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.\\n\\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `add_messages` function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\n#### Serialization\\n\\nIn addition to keeping track of message IDs, the `add_messages` function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format:\\n\\n```python\\n# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}\\n\\n# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n```\\n\\nSince the state updates are always deserialized into LangChain `Messages` when using `add_messages`, you should use dot notation to access message attributes, like `state[\"messages\"][-1].content`. Below is an example of a graph that uses `add_messages` as its reducer function.\\n\\n```python\\nfrom langchain_core.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n```\\n\\n#### MessagesState\\n\\nSince having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the `add_messages` reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\n\\n```python\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n```\\n\\n## Nodes\\n\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\n1. `state`: The [state](#state) of the graph\\n2. `config`: A `RunnableConfig` object that contains configuration information like `thread_id` and tracing information like `tags`\\n3. `runtime`: A `Runtime` object that contains [runtime `context`](#runtime-context) and other information like `store` and `stream_writer`\\n\\nSimilar to `NetworkX`, you add these nodes to a graph using the [add_node](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_node) method:\\n\\n```python\\nfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...\\n```\\n\\nBehind the scenes, functions are converted to [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)s, which add batch and async support to your function, along with native tracing and debugging.\\n\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\n\\n```python\\nbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n```\\n\\n### `START` Node\\n\\nThe `START` Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\n\\n```python\\nfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n```\\n\\n### `END` Node\\n\\nThe `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\n\\n```python\\nfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)\\n```\\n\\n### Node Caching\\n\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\n- Specify a cache when compiling a graph (or specifying an entrypoint)\\n- Specify a cache policy for nodes. Each cache policy supports:\\n  - `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.\\n  - `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\nFor example:\\n\\n```python\\nimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\nbuilder = StateGraph(State)\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}\\n\\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))  # (1)!\\n[{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))  # (2)!\\n[{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n```\\n\\n1. First run takes two seconds to run (due to mocked expensive computation).\\n2. Second run utilizes cache and returns quickly.\\n\\n## Edges\\n\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\\n\\n- Normal Edges: Go directly from one node to the next.\\n- Conditional Edges: Call a function to determine which node(s) to go to next.\\n- Entry Point: Which node to call first when user input arrives.\\n- Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\n### Normal Edges\\n\\nIf you **always** want to go from node A to node B, you can use the [add_edge](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly.\\n\\n```python\\ngraph.add_edge(\"node_a\", \"node_b\")\\n```\\n\\n### Conditional Edges\\n\\nIf you want to **optionally** route to 1 or more edges (or optionally terminate), you can use the [add_conditional_edges](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a \"routing function\" to call after that node is executed:\\n\\n```python\\ngraph.add_conditional_edges(\"node_a\", routing_function)\\n```\\n\\nSimilar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.\\n\\nBy default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\n\\nYou can optionally provide a dictionary that maps the `routing_function`\\'s output to the name of the next node.\\n\\n```python\\ngraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n```\\n\\n!!! tip\\n\\nUse [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\n### Entry Point\\n\\nThe entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph.\\n\\n```python\\nfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n```\\n\\n### Conditional Entry Point\\n\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to accomplish this.\\n\\n```python\\nfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n```\\n\\nYou can optionally provide a dictionary that maps the `routing_function`\\'s output to the name of the next node.\\n\\n```python\\ngraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\\n```\\n\\n## `Send`\\n\\nBy default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object).\\n\\nTo support this design pattern, LangGraph supports returning [`Send`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node.\\n\\n```python\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]\\n\\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n```\\n\\n## `Command`\\n\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [`Command`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions:\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n```\\n\\nWith `Command` you can also achieve dynamic control flow behavior (identical to [conditional edges](#conditional-edges)):\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\\n```\\n\\n!!! important\\n\\nWhen returning `Command` in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal[\"my_other_node\"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`.\\n\\nCheck out this [how-to guide](../how-tos/graph-api.md#combine-control-flow-and-state-updates-with-command) for an end-to-end example of how to use `Command`.\\n\\n### When should I use Command instead of conditional edges?\\n\\n- Use `Command` when you need to **both** update the graph state **and** route to a different node. For example, when implementing [multi-agent handoffs](./multi_agent.md#handoffs) where it\\'s important to route to a different agent and pass some information to that agent.\\n- Use [conditional edges](#conditional-edges) to route between nodes conditionally without updating the state.\\n\\n### Navigating to a node in a parent graph\\n\\nIf you are using [subgraphs](./subgraphs.md), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n```\\n\\n!!! note\\n\\nSetting `graph` to `Command.PARENT` will navigate to the closest parent graph.\\n\\n!!! important \"State updates with `Command.PARENT`\"\\n\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you\\'re updating in the parent graph state. See this [example](../how-tos/graph-api.md#navigate-to-a-node-in-a-parent-graph).\\n\\nThis is particularly useful when implementing [multi-agent handoffs](./multi_agent.md#handoffs).\\n\\nCheck out [this guide](../how-tos/graph-api.md#navigate-to-a-node-in-a-parent-graph) for detail.\\n\\n### Using inside tools\\n\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\n\\nRefer to [this guide](../how-tos/graph-api.md#use-inside-tools) for detail.\\n\\n### Human-in-the-loop\\n\\n`Command` is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, `Command` is then used to supply the input and resume execution via `Command(resume=\"User input\")`. Check out [this conceptual guide](./human_in_the_loop.md) for more information.\\n\\n## Graph Migrations\\n\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\\n\\n- For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\n- For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.\\n- For modifying state, we have full backwards and forwards compatibility for adding and removing keys\\n- State keys that are renamed lose their saved state in existing threads\\n- State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.\\n\\n## Runtime Context\\n\\nWhen creating a graph, you can specify a `context_schema` for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\n\\n```python\\n@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n```\\n\\nYou can then pass this context into the graph using the `context` parameter of the `invoke` method.\\n\\n```python\\ngraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n```\\n\\nYou can then access and use this context inside a node or conditional edge:\\n\\n```python\\nfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    ...\\n```\\n\\nSee [this guide](../how-tos/graph-api.md#add-runtime-configuration) for a full breakdown on configuration.\\n:::\\n\\n### Recursion Limit', source='langgraph-llms-full.txt', metadata={'score': 0.6947125196456909, 'fallback': True, 'confidence': 0.9})"
  ],
  "final_answer": "Here's how to handle retries in LangGraph nodes, addressing the key aspects:\n\n**1. What types of errors or exceptions are you anticipating needing to retry?**\n\n*   **Server Errors (5xx):** These indicate problems on the server-side, making retries a reasonable approach.\n*   **Client Errors (4xx):**  Specifically, `429 Too Many Requests` is a common scenario where retrying after a delay is effective. Other 4xx errors might be retryable depending on the specific error and its cause.\n\n**2. What retry strategies are most appropriate for your use case?**\n\nWhile the documentation doesn't explicitly detail retry strategies, common strategies applicable to LangGraph nodes include:\n\n*   **Exponential Backoff:** Gradually increase the delay between retry attempts. This is suitable for handling temporary service disruptions or rate limiting.\n*   **Fixed Delay:** Retry after a consistent delay.  Appropriate when the error is expected to resolve quickly.\n*   **Retry with Jitter:** Add a random element to the delay to avoid multiple clients retrying simultaneously.\n\n**3. How do you implement retry logic within a LangGraph node's function or callable?**\n\nLangGraph leverages the `langchain_core.runnables.retry.RunnableRetry` class (or similar mechanisms) to handle retries.  While there isn't a direct example in the provided documentation *explicitly* showing its usage *within* a node's function, the principle is to wrap the potentially failing operation with retry logic.\n\nHere's a conceptual example using `RunnableRetry`:\n\n```python\nfrom langchain_core.runnables.retry import RunnableRetry\n\ndef my_node(state):\n    def _potentially_failing_operation():\n        # Your code that might raise an exception (e.g., API call)\n        try:\n            result = make_api_call(state[\"input\"])\n            return result\n        except Exception as e:\n            raise e  # Re-raise the exception for RunnableRetry to handle\n\n    retryable = RunnableRetry(func=_potentially_failing_operation) # Wrap the function\n    result = retryable.invoke() # Invoke the retryable function\n    return {\"output\": result}\n```\n\n**4. How can you configure retry behavior (e.g., maximum number of retries, delay between retries) in a centralized and manageable way?**\n\nThe `RunnableRetry` class allows configuration of retry behavior.  You can set parameters like:\n\n*   `stop_after_attempt`:  The maximum number of retry attempts.\n*   `wait_fixed`: A fixed delay between retries (in seconds).\n*   `wait_exponential_multiplier`: Multiplier for exponential backoff.\n*   `wait_exponential_max`: Maximum delay for exponential backoff.\n*   `retry_on_exception`: A function that determines whether to retry based on the exception raised.\n\nExample:\n\n```python\nfrom langchain_core.runnables.retry import RunnableRetry\nimport time\n\ndef my_node(state):\n    def _potentially_failing_operation():\n        # Your code that might raise an exception (e.g., API call)\n        try:\n            result = make_api_call(state[\"input\"])\n            return result\n        except Exception as e:\n            raise e  # Re-raise the exception for RunnableRetry to handle\n\n    retryable = RunnableRetry(\n        func=_potentially_failing_operation,\n        stop_after_attempt=3,\n        wait_fixed=1,\n        retry_on_exception=lambda e: isinstance(e, ValueError) # Retry only on ValueErrors\n    )\n    result = retryable.invoke()\n    return {\"output\": result}\n```\n\n**5. How do you handle state persistence or data loss across retries within a LangGraph flow?**\n\n*   **Tasks and Checkpointing:** The most robust approach is to encapsulate the retryable operation within a `@task`.  Tasks are designed to be checkpointed, meaning their results are saved. If a task fails and the workflow resumes, the task will be re-executed, but LangGraph will retrieve the persisted result if it exists, preventing redundant API calls.\n*   **Idempotency:** Ensure that the operations being retried are idempotent.  An idempotent operation produces the same result no matter how many times it's executed. This prevents unintended side effects from multiple retries.\n*   **State Management:** If the node modifies the graph's state, ensure that the retry logic accounts for any partial updates that might have occurred before the failure.\n\n**6. How do you monitor and log retry attempts for debugging and observability?**\n\n*   **LangSmith Tracing:** LangGraph integrates with LangSmith for tracing.  Enable tracing to monitor the execution of your graph, including retry attempts.  LangSmith provides detailed information about each run, including any exceptions that occurred and the number of retries.\n*   **Custom Logging:**  Within the retry logic (e.g., inside the `_potentially_failing_operation` function), add logging statements to record retry attempts, the exceptions encountered, and any relevant context.\n\nHere's an example of adding logging:\n\n```python\nimport logging\nfrom langchain_core.runnables.retry import RunnableRetry\n\nlogger = logging.getLogger(__name__)\n\ndef my_node(state):\n    def _potentially_failing_operation():\n        try:\n            result = make_api_call(state[\"input\"])\n            return result\n        except Exception as e:\n            logger.warning(f\"API call failed: {e}. Retrying...\")\n            raise e\n\n    retryable = RunnableRetry(func=_potentially_failing_operation, stop_after_attempt=3)\n    result = retryable.invoke()\n    return {\"output\": result}\n```\n\nIn summary, handling retries in LangGraph involves using `RunnableRetry` (or similar), encapsulating operations in tasks for checkpointing, ensuring idempotency, and leveraging LangSmith and custom logging for monitoring and debugging.",
  "reasoning_style": "hybrid",
  "orc_plan": "Okay, here's a breakdown of how to approach handling retries in LangGraph nodes, broken down into sub-questions:\n\n1.  What types of errors or exceptions are you anticipating needing to retry?\n2.  What retry strategies are most appropriate for your use case (e.g., exponential backoff, fixed delay, etc.)?\n3.  How do you implement retry logic within a LangGraph node's function or callable?\n4.  How can you configure retry behavior (e.g., maximum number of retries, delay between retries) in a centralized and manageable way?\n5.  How do you handle state persistence or data loss across retries within a LangGraph flow?\n6.  How do you monitor and log retry attempts for debugging and observability?",
  "orc_subquestions": [
    "What types of errors or exceptions are you anticipating needing to retry?",
    "What retry strategies are most appropriate for your use case (e.g., exponential backoff, fixed delay, etc.)?",
    "How do you implement retry logic within a LangGraph node's function or callable?",
    "How can you configure retry behavior (e.g., maximum number of retries, delay between retries) in a centralized and manageable way?",
    "How do you handle state persistence or data loss across retries within a LangGraph flow?",
    "How do you monitor and log retry attempts for debugging and observability?"
  ]
}