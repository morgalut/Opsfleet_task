{
  "question": "How do I handle errors and retries in LangGraph nodes?",
  "mode": "online",
  "retrieved_docs": [
    "Document(content='class State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)\\n\\ngraph = builder.compile()\\n```\\n\\n```python\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n![Conditional branching graph](assets/graph_api_image_5.png)\\n\\n```python\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n```\\n\\n```\\nAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n```\\n\\n!!! tip\\n\\nYour conditional edges can route to multiple destination nodes. For example:\\n\\n```python\\n    def route_bc_or_cd(state: State) -> Sequence[str]:\\n        if state[\"which\"] == \"cd\":\\n            return [\"c\", \"d\"]\\n        return [\"b\", \"c\"]\\n    ```\\n\\n## Map-Reduce and the Send API\\n\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\n\\n```python\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: Annotated[list[str], operator.add]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\ngraph = builder.compile()\\n```\\n\\n```python\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n![Map-reduce graph with fanout](assets/graph_api_image_6.png)\\n\\n```python\\n# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n```\\n\\n```\\n{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n```\\n\\n## Create and control loops\\n\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](../concepts/low_level.md#conditional-edges) that routes to the [END](../concepts/low_level.md#end-node) node once we reach some termination condition.\\n\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](../concepts/low_level.md#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](../concepts/low_level.md#recursion-limit).\\n\\nLet\\'s consider a simple graph with a loop to better understand how these mechanisms work.\\n\\n!!! tip\\n\\nTo return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).\\n\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\n\\n```python\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n```\\n\\nTo control the recursion limit, specify `\"recursionLimit\"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:\\n\\n```python\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n```\\n\\nLet\\'s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\n\\n```python\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n```\\n\\n```python\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n![Simple loop graph](assets/graph_api_image_7.png)\\n\\nThis architecture is similar to a [React agent](../agents/overview.md) in which node `\"a\"` is a tool-calling model, and node `\"b\"` represents the tools.\\n\\nIn our `route` conditional edge, we specify that we should end after the `\"aggregate\"` list in the state passes a threshold length.\\n\\nInvoking the graph, we see that we alternate between nodes `\"a\"` and `\"b\"` before terminating once we reach the termination condition.\\n\\n```python\\ngraph.invoke({\"aggregate\": []})\\n```\\n\\n```\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n```\\n\\n### Impose a recursion limit\\n\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\\'s [recursion limit](../concepts/low_level.md#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](../concepts/low_level.md#graphs). We can then catch and handle this exception:\\n\\n```python\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n```\\n\\n```\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n```\\n\\n??? example \"Extended example: return state on hitting recursion limit\"\\n\\nInstead of raising `GraphRecursionError`, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\\n\\nLangGraph implements a special `RemainingSteps` annotation. Under the hood, it creates a `ManagedValue` channel -- a state channel that will exist for the duration of our graph run and no longer.\\n\\n```python\\n    import operator\\n    from typing import Annotated, Literal\\n    from typing_extensions import TypedDict\\n    from langgraph.graph import StateGraph, START, END\\n    from langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n        aggregate: Annotated[list, operator.add]\\n        remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n        print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n        return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n        print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n        return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\n    builder = StateGraph(State)\\n    builder.add_node(a)\\n    builder.add_node(b)\\n\\n# Define edges\\n    def route(state: State) -> Literal[\"b\", END]:\\n        if state[\"remaining_steps\"] <= 2:\\n            return END\\n        else:\\n            return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\n    builder.add_conditional_edges(\"a\", route)\\n    builder.add_edge(\"b\", \"a\")\\n    graph = builder.compile()\\n\\n# Test it out\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\n    print(result)\\n    ```\\n    ```\\n    Node A sees []\\n    Node B sees [\\'A\\']\\n    Node A sees [\\'A\\', \\'B\\']\\n    {\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n    ```\\n\\n??? example \"Extended example: loops with branches\"\\n\\nTo better understand how the recursion limit works, let\\'s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\\n\\n```python\\n    import operator\\n    from typing import Annotated, Literal\\n    from typing_extensions import TypedDict\\n    from langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n        aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n        print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n        return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n        print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n        return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n        print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n        return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n        print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n        return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\n    builder = StateGraph(State)\\n    builder.add_node(a)\\n    builder.add_node(b)\\n    builder.add_node(c)\\n    builder.add_node(d)\\n\\n# Define edges\\n    def route(state: State) -> Literal[\"b\", END]:\\n        if len(state[\"aggregate\"]) < 7:\\n            return \"b\"\\n        else:\\n            return END\\n\\nbuilder.add_edge(START, \"a\")\\n    builder.add_conditional_edges(\"a\", route)\\n    builder.add_edge(\"b\", \"c\")\\n    builder.add_edge(\"b\", \"d\")\\n    builder.add_edge([\"c\", \"d\"], \"a\")\\n    graph = builder.compile()\\n    ```\\n\\n```python\\n    from IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n    ```\\n\\n![Complex loop graph with branches](assets/graph_api_image_8.png)\\n\\nThis graph looks complex, but can be conceptualized as loop of [supersteps](../concepts/low_level.md#graphs):\\n\\n1. Node A\\n    2. Node B\\n    3. Nodes C and D\\n    4. Node A\\n    5. ...\\n\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.\\n\\nInvoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\\n\\n```python\\n    result = graph.invoke({\"aggregate\": []})\\n    ```\\n    ```\\n    Node A sees []\\n    Node B sees [\\'A\\']\\n    Node D sees [\\'A\\', \\'B\\']\\n    Node C sees [\\'A\\', \\'B\\']\\n    Node A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n    Node B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\n    Node D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\n    Node C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\n    Node A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n    ```\\n\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\\n\\n```python\\n    from langgraph.errors import GraphRecursionError\\n\\ntry:\\n        result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\n    except GraphRecursionError:\\n        print(\"Recursion Error\")\\n    ```\\n    ```\\n    Node A sees []\\n    Node B sees [\\'A\\']\\n    Node C sees [\\'A\\', \\'B\\']\\n    Node D sees [\\'A\\', \\'B\\']\\n    Node A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n    Recursion Error\\n    ```\\n\\n## Async\\n\\nUsing the async programming paradigm can produce significant performance improvements when running [IO-bound](https://en.wikipedia.org/wiki/I/O_bound) code concurrently (e.g., making concurrent API requests to a chat model provider).\\n\\nTo convert a `sync` implementation of the graph to an `async` implementation, you will need to:\\n\\n1. Update `nodes` use `async def` instead of `def`.\\n2. Update the code inside to use `await` appropriately.\\n3. Invoke the graph with `.ainvoke` or `.astream` as desired.\\n\\nBecause many LangChain objects implement the [Runnable Protocol](https://python.langchain.com/docs/expression_language/interface/) which has `async` variants of all the `sync` methods it\\'s typically fairly quick to upgrade a `sync` graph to an `async` graph.\\n\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\n\\n{% include-markdown \"../../snippets/chat_model_tabs.md\" %}\\n\\n```python hl_lines=\"4 5 12\"\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState): # (1)!\\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\\n```\\n\\n1. Declare nodes to be async functions.\\n2. Use async invocations when available within the node.\\n3. Use async invocations on the graph object itself.\\n\\n!!! tip \"Async streaming\"\\n\\nSee the [streaming guide](./streaming.md) for examples of streaming with async.\\n\\n## Combine control flow and state updates with `Command`\\n\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [Command](../reference/types.md#langgraph.types.Command) object from node functions:\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n```\\n\\nWe show an end-to-end example below. Let\\'s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\n\\n```python\\nimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"b\", \"c\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"b\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n# note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n```\\n\\nWe can now create the `StateGraph` with the above nodes. Notice that the graph doesn\\'t have [conditional edges](../concepts/low_level.md#conditional-edges) for routing! This is because control flow is defined with `Command` inside `node_a`.\\n\\n```python\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n```\\n\\n!!! important\\n\\nYou might have noticed that we used `Command` as a return type annotation, e.g. `Command[Literal[\"node_b\", \"node_c\"]]`. This is necessary for the graph rendering and tells LangGraph that `node_a` can navigate to `node_b` and `node_c`.\\n\\n```python\\nfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n```\\n\\n![Command-based graph navigation](assets/graph_api_image_11.png)\\n\\nIf we run the graph multiple times, we\\'d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\n\\n```python\\ngraph.invoke({\"foo\": \"\"})\\n```\\n\\n```\\nCalled A\\nCalled C\\n```\\n\\n### Navigate to a node in a parent graph\\n\\nIf you are using [subgraphs](../concepts/subgraphs.md), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n```\\n\\nLet\\'s demonstrate this using the above example. We\\'ll do so by changing `nodeA` in the above example into a single-node graph that we\\'ll add as a subgraph to our parent graph.\\n\\n!!! important \"State updates with `Command.PARENT`\"\\n\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph [state schemas](../concepts/low_level.md#schema), you **must** define a [reducer](../concepts/low_level.md#reducers) for the key you\\'re updating in the parent graph state. See the example below.\\n\\n```python hl_lines=\"6 23 33 37\"\\nimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]\\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n# note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,\\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n```\\n\\n```python\\ngraph.invoke({\"foo\": \"\"})\\n```\\n\\n```\\nCalled A\\nCalled C\\n```\\n\\n### Use inside tools\\n\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return `Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]})` from the tool:\\n\\n```python\\n@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n```\\n\\n!!! important\\n\\nYou MUST include `messages` (or any state key used for the message history) in `Command.update` when returning `Command` from a tool and the list of messages in `messages` MUST contain a `ToolMessage`. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\n\\nIf you are using tools that update state via `Command`, we recommend using prebuilt [`ToolNode`](../reference/agents.md#langgraph.prebuilt.tool_node.ToolNode) which automatically handles tools returning `Command` objects and propagates them to the graph state. If you\\'re writing a custom node that calls tools, you would need to manually propagate `Command` objects returned by the tools as the update from the node.\\n\\n## Visualize your graph\\n\\nHere we demonstrate how to visualize the graphs you create.\\n\\nYou can visualize any arbitrary [Graph](https://langchain-ai.github.io/langgraph/reference/graphs/), including [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.state.StateGraph).\\n\\nLet\\'s have some fun by drawing fractals :).\\n\\n```python\\nimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\\n    if len(state[\"messages\"]) > 10:\\n        return \"__end__\"\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, \"__end__\")\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n```\\n\\n### Mermaid\\n\\nWe can also convert a graph class into Mermaid syntax.\\n\\n```python\\nprint(app.get_graph().draw_mermaid())\\n```\\n\\n```\\n%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n\\t__start__([<p>__start__</p>]):::first\\n\\tentry_node(entry_node)\\n\\tnode_entry_node_A(node_entry_node_A)\\n\\tnode_entry_node_B(node_entry_node_B)\\n\\tnode_node_entry_node_B_A(node_node_entry_node_B_A)\\n\\tnode_node_entry_node_B_B(node_node_entry_node_B_B)\\n\\tnode_node_entry_node_B_C(node_node_entry_node_B_C)\\n\\t__end__([<p>__end__</p>]):::last\\n\\t__start__ --> entry_node;\\n\\tentry_node --> __end__;\\n\\tentry_node --> node_entry_node_A;\\n\\tentry_node --> node_entry_node_B;\\n\\tnode_entry_node_B --> node_node_entry_node_B_A;\\n\\tnode_entry_node_B --> node_node_entry_node_B_B;\\n\\tnode_entry_node_B --> node_node_entry_node_B_C;\\n\\tnode_entry_node_A -.-> entry_node;\\n\\tnode_entry_node_A -.-> __end__;\\n\\tnode_node_entry_node_B_A -.-> entry_node;\\n\\tnode_node_entry_node_B_A -.-> __end__;\\n\\tnode_node_entry_node_B_B -.-> entry_node;\\n\\tnode_node_entry_node_B_B -.-> __end__;\\n\\tnode_node_entry_node_B_C -.-> entry_node;\\n\\tnode_node_entry_node_B_C -.-> __end__;\\n\\tclassDef default fill:#f2f0ff,line-height:1.2\\n\\tclassDef first fill-opacity:0\\n\\tclassDef last fill:#bfb6fc\\n```\\n\\n### PNG\\n\\nIf preferred, we could render the Graph into a `.png`. Here we could use three options:\\n\\n- Using Mermaid.ink API (does not require additional packages)\\n- Using Mermaid + Pyppeteer (requires `pip install pyppeteer`)\\n- Using graphviz (which requires `pip install graphviz`)\\n\\n**Using Mermaid.Ink**\\n\\nBy default, `draw_mermaid_png()` uses Mermaid.Ink\\'s API to generate the diagram.\\n\\n```python\\nfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n```\\n\\n![Fractal graph visualization](assets/graph_api_image_10.png)\\n\\n**Using Mermaid + Pyppeteer**\\n\\n```python\\nimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n```\\n\\n**Using Graphviz**\\n\\n```python\\ntry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n```\\n\\n---\\nhow-tos/streaming.md\\n---\\n\\n# Stream outputs\\n\\nYou can [stream outputs](../concepts/streaming.md) from a LangGraph agent or workflow.\\n\\n## Supported stream modes\\n\\nPass one or more of the following stream modes as a list to the [`stream()`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream()`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods:', source='langgraph-llms-full.txt', metadata={'score': 0.5765028595924377, 'fallback': True, 'confidence': 0.9})",
    "Document(content='```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nmy_workflow.invoke(Command(resume=some_resume_value), config)\\n    ```\\n\\n=== \"Async Invoke\"\\n\\n```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nawait my_workflow.ainvoke(Command(resume=some_resume_value), config)\\n    ```\\n\\n=== \"Stream\"\\n\\n```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nfor chunk in my_workflow.stream(Command(resume=some_resume_value), config):\\n        print(chunk)\\n    ```\\n\\n=== \"Async Stream\"\\n\\n```python\\n    from langgraph.types import Command\\n\\nconfig = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nasync for chunk in my_workflow.astream(Command(resume=some_resume_value), config):\\n        print(chunk)\\n    ```\\n\\n**Resuming after an error**\\n\\nTo resume after an error, run the `entrypoint` with a `None` and the same **thread id** (config).\\n\\nThis assumes that the underlying **error** has been resolved and execution can proceed successfully.\\n\\n=== \"Invoke\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nmy_workflow.invoke(None, config)\\n    ```\\n\\n=== \"Async Invoke\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nawait my_workflow.ainvoke(None, config)\\n    ```\\n\\n=== \"Stream\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nfor chunk in my_workflow.stream(None, config):\\n        print(chunk)\\n    ```\\n\\n=== \"Async Stream\"\\n\\n```python\\n    config = {\\n        \"configurable\": {\\n            \"thread_id\": \"some_thread_id\"\\n        }\\n    }\\n\\nasync for chunk in my_workflow.astream(None, config):\\n        print(chunk)\\n    ```\\n\\n### Short-term memory\\n\\nWhen an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in [checkpoints](persistence.md#checkpoints).\\n\\nThis allows accessing the state from the previous invocation using the `previous` parameter.\\n\\nBy default, the `previous` parameter is the return value of the previous invocation.\\n\\n```python\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> int:\\n    previous = previous or 0\\n    return number + previous\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"some_thread_id\"\\n    }\\n}\\n\\nmy_workflow.invoke(1, config)  # 1 (previous was None)\\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)\\n```\\n\\n#### `entrypoint.final`\\n\\n[`entrypoint.final`](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.entrypoint.final) is a special primitive that can be returned from an entrypoint and allows **decoupling** the value that is **saved in the checkpoint** from the **return value of the entrypoint**.\\n\\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is `entrypoint.final[return_type, save_type]`.\\n\\n```python\\n@entrypoint(checkpointer=checkpointer)\\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\\n    previous = previous or 0\\n    # This will return the previous value to the caller, saving\\n    # 2 * number to the checkpoint, which will be used in the next invocation\\n    # for the `previous` parameter.\\n    return entrypoint.final(value=previous, save=2 * number)\\n\\nconfig = {\\n    \"configurable\": {\\n        \"thread_id\": \"1\"\\n    }\\n}\\n\\nmy_workflow.invoke(3, config)  # 0 (previous was None)\\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\\n```\\n\\n## Task\\n\\nA **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\\n\\n- **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\\n- **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](persistence.md) for more details).\\n\\n### Definition\\n\\nTasks are defined using the `@task` decorator, which wraps a regular Python function.\\n\\n```python\\nfrom langgraph.func import task\\n\\n@task()\\ndef slow_computation(input_value):\\n    # Simulate a long-running operation\\n    ...\\n    return result\\n```\\n\\n!!! important \"Serialization\"\\n\\nThe **outputs** of tasks must be JSON-serializable to support checkpointing.\\n\\n### Execution\\n\\n**Tasks** can only be called from within an **entrypoint**, another **task**, or a [state graph node](./low_level.md#nodes).\\n\\nTasks _cannot_ be called directly from the main application code.\\n\\nWhen you call a **task**, it returns _immediately_ with a future object. A future is a placeholder for a result that will be available later.\\n\\nTo obtain the result of a **task**, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`).\\n\\n=== \"Synchronous Invocation\"\\n\\n```python\\n    @entrypoint(checkpointer=checkpointer)\\n    def my_workflow(some_input: int) -> int:\\n        future = slow_computation(some_input)\\n        return future.result()  # Wait for the result synchronously\\n    ```\\n\\n=== \"Asynchronous Invocation\"\\n\\n```python\\n    @entrypoint(checkpointer=checkpointer)\\n    async def my_workflow(some_input: int) -> int:\\n        return await slow_computation(some_input)  # Await result asynchronously\\n    ```\\n\\n## When to use a task\\n\\n**Tasks** are useful in the following scenarios:\\n\\n- **Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don\\'t need to recompute it when resuming the workflow.\\n- **Human-in-the-loop**: If you\\'re building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.\\n- **Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\\n- **Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.smith.langchain.com/).\\n- **Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.\\n\\n## Serialization\\n\\nThere are two key aspects to serialization in LangGraph:\\n\\n1. `entrypoint` inputs and outputs must be JSON-serializable.\\n2. `task` outputs must be JSON-serializable.\\n\\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\\n\\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\\n\\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\\n\\n## Determinism\\n\\nTo utilize features like **human-in-the-loop**, any randomness should be encapsulated inside of **tasks**. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same _sequence of steps_, even if **task** results are non-deterministic.\\n\\nLangGraph achieves this behavior by persisting **task** and [**subgraph**](./subgraphs.md) results as they execute. A well-designed workflow ensures that resuming execution follows the _same sequence of steps_, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running **tasks** or **tasks** with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\\n\\nWhile different runs of a workflow can produce different results, resuming a **specific** run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up **task** and **subgraph** results that were executed prior to the graph being interrupted and avoid recomputing them.\\n\\n## Idempotency\\n\\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside **tasks** functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a **task** starts, but does not complete successfully. Then, if the workflow is resumed, the **task** will run again. Use idempotency keys or verify existing results to avoid duplication.\\n\\n## Common Pitfalls\\n\\n### Handling side effects\\n\\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\\n\\n=== \"Incorrect\"\\n\\nIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.\\n\\n```python hl_lines=\"5 6\"\\n    @entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        # This code will be executed a second time when resuming the workflow.\\n        # Which is likely not what you want.\\n        with open(\"output.txt\", \"w\") as f:\\n            f.write(\"Side effect executed\")\\n        value = interrupt(\"question\")\\n        return value\\n    ```\\n\\n=== \"Correct\"\\n\\nIn this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.\\n\\n```python hl_lines=\"3 4\"\\n    from langgraph.func import task\\n\\n@task\\n    def write_to_file():\\n        with open(\"output.txt\", \"w\") as f:\\n            f.write(\"Side effect executed\")\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        # The side effect is now encapsulated in a task.\\n        write_to_file().result()\\n        value = interrupt(\"question\")\\n        return value\\n    ```\\n\\n### Non-deterministic control flow\\n\\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\\n\\n- In a task: Get random number (5) → interrupt → resume → (returns 5 again) → ...\\n- Not in a task: Get random number (5) → interrupt → resume → get new random number (7) → ...\\n\\nThis is especially important when using **human-in-the-loop** workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it\\'s matched with the corresponding resume value. This matching is strictly **index-based**, so the order of the resume values should match the order of the interrupts.\\n\\nIf order of execution is not maintained when resuming, one `interrupt` call may be matched with the wrong `resume` value, leading to incorrect results.\\n\\nPlease read the section on [determinism](#determinism) for more details.\\n\\n=== \"Incorrect\"\\n\\nIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.\\n\\n```python hl_lines=\"6\"\\n    from langgraph.func import entrypoint\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        t0 = inputs[\"t0\"]\\n        t1 = time.time()\\n\\ndelta_t = t1 - t0\\n\\nif delta_t > 1:\\n            result = slow_task(1).result()\\n            value = interrupt(\"question\")\\n        else:\\n            result = slow_task(2).result()\\n            value = interrupt(\"question\")\\n\\nreturn {\\n            \"result\": result,\\n            \"value\": value\\n        }\\n    ```\\n\\n=== \"Correct\"\\n\\nIn this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.\\n\\n```python hl_lines=\"5 6 12\"\\n    import time\\n\\nfrom langgraph.func import task\\n\\n@task\\n    def get_time() -> float:\\n        return time.time()\\n\\n@entrypoint(checkpointer=checkpointer)\\n    def my_workflow(inputs: dict) -> int:\\n        t0 = inputs[\"t0\"]\\n        t1 = get_time().result()\\n\\ndelta_t = t1 - t0\\n\\nif delta_t > 1:\\n            result = slow_task(1).result()\\n            value = interrupt(\"question\")\\n        else:\\n            result = slow_task(2).result()\\n            value = interrupt(\"question\")\\n\\nreturn {\\n            \"result\": result,\\n            \"value\": value\\n        }\\n    ```\\n\\n---\\nconcepts/deployment_options.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Deployment Options\\n\\n## Free deployment\\n\\n[Local](../tutorials/langgraph-platform/local-server.md): Deploy for local testing and development.\\n\\n## Production deployment\\n\\nThere are 4 main options for deploying with the [LangGraph Platform](langgraph_platform.md):\\n\\n1. [Cloud SaaS](#cloud-saas)\\n\\n1. [Self-Hosted Data Plane](#self-hosted-data-plane)\\n\\n1. [Self-Hosted Control Plane](#self-hosted-control-plane)\\n\\n1. [Standalone Container](#standalone-container)\\n\\nA quick comparison:\\n\\n|                      | **Cloud SaaS** | **Self-Hosted Data Plane** | **Self-Hosted Control Plane** | **Standalone Container** |\\n|----------------------|----------------|----------------------------|-------------------------------|--------------------------|\\n| **[Control plane UI/API](../concepts/langgraph_control_plane.md)** | Yes | Yes | Yes | No |\\n| **CI/CD** | Managed internally by platform | Managed externally by you | Managed externally by you | Managed externally by you |\\n| **Data/compute residency** | LangChain\\'s cloud | Your cloud | Your cloud | Your cloud |\\n| **LangSmith compatibility** | Trace to LangSmith SaaS | Trace to LangSmith SaaS | Trace to Self-Hosted LangSmith | Optional tracing |\\n| **[Pricing](https://www.langchain.com/pricing-langgraph-platform)** | Plus | Enterprise | Enterprise | Enterprise |\\n\\n## Cloud SaaS\\n\\nThe [Cloud SaaS](./langgraph_cloud.md) deployment option is a fully managed model for deployment where we manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in our cloud. This option provides a simple way to deploy and manage your LangGraph Servers.\\n\\nConnect your GitHub repositories to the platform and deploy your LangGraph Servers from the [control plane UI](./langgraph_control_plane.md#control-plane-ui). The build process (i.e. CI/CD) is managed internally by the platform.\\n\\nFor more information, please see:\\n\\n* [Cloud SaaS Conceptual Guide](./langgraph_cloud.md)\\n* [How to deploy to Cloud SaaS](../cloud/deployment/cloud.md)\\n\\n## Self-Hosted Data Plane\\n\\n!!! info \"Important\"\\n    The Self-Hosted Data Plane deployment option requires an [Enterprise](../concepts/plans.md) plan.\\n\\nThe [Self-Hosted Data Plane](./langgraph_self_hosted_data_plane.md) deployment option is a \"hybrid\" model for deployment where we manage the [control plane](./langgraph_control_plane.md) in our cloud and you manage the [data plane](./langgraph_data_plane.md) in your cloud. This option provides a way to securely manage your data plane infrastructure, while offloading control plane management to us.\\n\\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server from the [control plane UI](./langgraph_control_plane.md#control-plane-ui).\\n\\nSupported Compute Platforms: [Kubernetes](https://kubernetes.io/), [Amazon ECS](https://aws.amazon.com/ecs/) (coming soon!)\\n\\nFor more information, please see:\\n\\n* [Self-Hosted Data Plane Conceptual Guide](./langgraph_self_hosted_data_plane.md)\\n* [How to deploy the Self-Hosted Data Plane](../cloud/deployment/self_hosted_data_plane.md)\\n\\n## Self-Hosted Control Plane\\n\\n!!! info \"Important\"\\n    The Self-Hosted Control Plane deployment option requires an [Enterprise](../concepts/plans.md) plan.\\n\\nThe [Self-Hosted Control Plane](./langgraph_self_hosted_control_plane.md) deployment option is a fully self-hosted model for deployment where you manage the [control plane](./langgraph_control_plane.md) and [data plane](./langgraph_data_plane.md) in your cloud. This option gives you full control and responsibility of the control plane and data plane infrastructure.\\n\\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server from the [control plane UI](./langgraph_control_plane.md#control-plane-ui).\\n\\nSupported Compute Platforms: [Kubernetes](https://kubernetes.io/)\\n\\nFor more information, please see:\\n\\n* [Self-Hosted Control Plane Conceptual Guide](./langgraph_self_hosted_control_plane.md)\\n* [How to deploy the Self-Hosted Control Plane](../cloud/deployment/self_hosted_control_plane.md)\\n\\n## Standalone Container\\n\\nThe [Standalone Container](./langgraph_standalone_container.md) deployment option is the least restrictive model for deployment. Deploy standalone instances of a LangGraph Server in your cloud, using any of the [available](./plans.md) license options.\\n\\nBuild a Docker image using the [LangGraph CLI](./langgraph_cli.md) and deploy your LangGraph Server using the container deployment tooling of your choice. Images can be deployed to any compute platform.\\n\\nFor more information, please see:\\n\\n* [Standalone Container Conceptual Guide](./langgraph_standalone_container.md)\\n* [How to deploy a Standalone Container](../cloud/deployment/standalone_container.md)\\n\\n## Related\\n\\nFor more information, please see:\\n\\n* [LangGraph Platform plans](./plans.md)\\n* [LangGraph Platform pricing](https://www.langchain.com/langgraph-platform-pricing)\\n\\n---\\nconcepts/sdk.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph SDK\\n\\nLangGraph Platform provides a python SDK for interacting with [LangGraph Server](./langgraph_server.md).\\n\\n!!! tip \"Python SDK reference\"\\n\\nFor detailed information about the Python SDK, see [Python SDK reference docs](../cloud/reference/sdk/python_sdk_ref.md).\\n\\n## Installation\\n\\nYou can install the LangGraph SDK using the following command:\\n\\n```bash\\npip install langgraph-sdk\\n```\\n\\n## Python sync vs. async\\n\\nThe Python SDK provides both synchronous (`get_sync_client`) and asynchronous (`get_client`) clients for interacting with LangGraph Server:\\n\\n=== \"Sync\"\\n\\n```python\\n    from langgraph_sdk import get_sync_client\\n\\nclient = get_sync_client(url=..., api_key=...)\\n    client.assistants.search()\\n    ```\\n\\n=== \"Async\"\\n\\n```python\\n    from langgraph_sdk import get_client\\n\\nclient = get_client(url=..., api_key=...)\\n    await client.assistants.search()\\n    ```\\n\\n## Learn more\\n\\n- [Python SDK Reference](../cloud/reference/sdk/python_sdk_ref.md)\\n- [LangGraph CLI API Reference](../cloud/reference/cli.md)\\n\\n---\\nconcepts/langgraph_server.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Server\\n\\n**LangGraph Server** offers an API for creating and managing agent-based applications. It is built on the concept of [assistants](assistants.md), which are agents configured for specific tasks, and includes built-in [persistence](persistence.md#memory-store) and a **task queue**. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions.\\n\\nUse LangGraph Server to create and manage [assistants](assistants.md), [threads](./persistence.md#threads), [runs](./assistants.md#execution), [cron jobs](../cloud/concepts/cron_jobs.md), [webhooks](../cloud/concepts/webhooks.md), and more.\\n\\n!!! tip \"API reference\"\\n  \\n    For detailed information on the API endpoints and data models, see [LangGraph Platform API reference docs](../cloud/reference/api/api_ref.html).\\n\\n## Application structure\\n\\nTo deploy a LangGraph Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.\\n\\nRead the [application structure](./application_structure.md) guide to learn how to structure your LangGraph application for deployment.\\n\\n## Parts of a deployment\\n\\nWhen you deploy LangGraph Server, you are deploying one or more [graphs](#graphs), a database for [persistence](persistence.md), and a task queue.\\n\\n### Graphs\\n\\nWhen you deploy a graph with LangGraph Server, you are deploying a \"blueprint\" for an [Assistant](assistants.md).\\n\\nAn [Assistant](assistants.md) is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases\\nthat can be served by the same graph.\\n\\nUpon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph\\'s default configuration settings.\\n\\n!!! note\\n\\nWe often think of a graph as implementing an [agent](agentic_concepts.md), but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple\\n    chatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use [multiple agents](./multi_agent.md) working in tandem.\\n\\n### Persistence and task queue\\n\\nLangGraph Server leverages a database for [persistence](persistence.md) and a task queue.\\n\\nCurrently, only [Postgres](https://www.postgresql.org/) is supported as a database for LangGraph Server and [Redis](https://redis.io/) as the task queue.\\n\\nIf you\\'re deploying using [LangGraph Platform](./langgraph_cloud.md), these components are managed for you. If you\\'re deploying LangGraph Server on your own infrastructure, you\\'ll need to set up and manage these components yourself.\\n\\nPlease review the [deployment options](./deployment_options.md) guide for more information on how these components are set up and managed.\\n\\n## Learn more\\n\\n* LangGraph [Application Structure](./application_structure.md) guide explains how to structure your LangGraph application for deployment.\\n* The [LangGraph Platform API Reference](../cloud/reference/api/api_ref.html) provides detailed information on the API endpoints and data models.\\n\\n---\\nconcepts/langgraph_platform.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Platform\\n\\nDevelop, deploy, scale, and manage agents with **LangGraph Platform** — the purpose-built platform for long-running, agentic workflows.\\n\\n!!! tip \"Get started with LangGraph Platform\"\\n\\nCheck out the [LangGraph Platform quickstart](../tutorials/langgraph-platform/local-server.md) for instructions on how to use LangGraph Platform to run a LangGraph application locally.\\n\\n## Why use LangGraph Platform?\\n\\n<div align=\"center\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pfAQxBS5z88?si=XGS6Chydn6lhSO1S\" title=\"What is LangGraph Platform?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></div>\\n\\nLangGraph Platform makes it easy to get your agent running in production —  whether it’s built with LangGraph or another framework — so you can focus on your app logic, not infrastructure. Deploy with one click to get a live endpoint, and use our robust APIs and built-in task queues to handle production scale.\\n\\n- **[Streaming Support](../cloud/how-tos/streaming.md)**: As agents grow more sophisticated, they often benefit from streaming both token outputs and intermediate states back to the user. Without this, users are left waiting for potentially long operations with no feedback. LangGraph Server provides multiple streaming modes optimized for various application needs.\\n\\n- **[Background Runs](../cloud/how-tos/background_run.md)**: For agents that take longer to process (e.g., hours), maintaining an open connection can be impractical. The LangGraph Server supports launching agent runs in the background and provides both polling endpoints and webhooks to monitor run status effectively.\\n \\n- **Support for long runs**: Regular server setups often encounter timeouts or disruptions when handling requests that take a long time to complete. LangGraph Server’s API provides robust support for these tasks by sending regular heartbeat signals, preventing unexpected connection closures during prolonged processes.\\n\\n- **Handling Burstiness**: Certain applications, especially those with real-time user interaction, may experience \"bursty\" request loads where numerous requests hit the server simultaneously. LangGraph Server includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads.\\n\\n- **[Double-texting](../cloud/how-tos/interrupt_concurrent.md)**: In user-driven applications, it’s common for users to send multiple messages rapidly. This “double texting” can disrupt agent flows if not handled properly. LangGraph Server offers built-in strategies to address and manage such interactions.\\n\\n- **[Checkpointers and memory management](persistence.md#checkpoints)**: For agents needing persistence (e.g., conversation memory), deploying a robust storage solution can be complex. LangGraph Platform includes optimized [checkpointers](persistence.md#checkpoints) and a [memory store](persistence.md#memory-store), managing state across sessions without the need for custom solutions.\\n\\n- **[Human-in-the-loop support](../cloud/how-tos/human_in_the_loop_breakpoint.md)**: In many applications, users require a way to intervene in agent processes. LangGraph Server provides specialized endpoints for human-in-the-loop scenarios, simplifying the integration of manual oversight into agent workflows.\\n\\n- **[LangGraph Studio](./langgraph_studio.md)**: Enables visualization, interaction, and debugging of agentic systems that implement the LangGraph Server API protocol. Studio also integrates with LangSmith to enable tracing, evaluation, and prompt engineering.\\n\\n- **[Deployment](./deployment_options.md)**: There are four ways to deploy on LangGraph Platform: [Cloud SaaS](../concepts/langgraph_cloud.md), [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md), [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md), and [Standalone Container](../concepts/langgraph_standalone_container.md).\\n\\n---\\nconcepts/tracing.md\\n---\\n\\n# Tracing\\n\\nTraces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](../how-tos/enable-tracing.md). This enables you to do the following:\\n\\n- [Debug a locally running application](../cloud/how-tos/clone_traces_studio.md).\\n- [Evaluate the application performance](../agents/evals.md).\\n- [Monitor the application](https://docs.smith.langchain.com/observability/how_to_guides/dashboards).\\n\\nTo get started, sign up for a free account at [LangSmith](https://smith.langchain.com/).\\n\\n## Learn more\\n\\n- [Graph runs in LangSmith](../how-tos/run-id-langsmith.md)\\n- [LangSmith Observability quickstart](https://docs.smith.langchain.com/observability)\\n- [Trace with LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph)\\n- [Tracing conceptual guide](https://docs.smith.langchain.com/observability/concepts#traces)\\n\\n---\\nconcepts/memory.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Memory', source='langgraph-llms-full.txt', metadata={'score': 0.5552582740783691, 'fallback': True, 'confidence': 0.9})",
    "Document(content='All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the LangGraph Server unavailable.\\n\\n---\\nconcepts/low_level.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Graph API concepts\\n\\n## Graphs\\n\\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\\n\\n1. [`State`](#state): A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\\n\\n2. [`Nodes`](#nodes): Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\\n\\n3. [`Edges`](#edges): Functions that determine which `Node` to execute next based on the current state. They can be conditional branches or fixed transitions.\\n\\nBy composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: `Nodes` and `Edges` are nothing more than functions - they can contain an LLM or just good ol\\' code.\\n\\nIn short: _nodes do the work, edges tell what to do next_.\\n\\nLangGraph\\'s underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google\\'s [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete \"super-steps.\"\\n\\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or \"channels\"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit.\\n\\n### StateGraph\\n\\nThe `StateGraph` class is the main graph class to use. This is parameterized by a user defined `State` object.\\n\\n### Compiling your graph\\n\\nTo build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed?\\n\\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](./persistence.md) and breakpoints. You compile your graph by just calling the `.compile` method:\\n\\n```python\\ngraph = graph_builder.compile(...)\\n```\\n\\nYou **MUST** compile your graph before you can use it.\\n\\n## State\\n\\nThe first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.\\n\\n### Schema\\n\\nThe main documented way to specify the schema of a graph is by using a [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict). If you want to provide default values in your state, use a [`dataclass`](https://docs.python.org/3/library/dataclasses.html). We also support using a Pydantic [BaseModel](../how-tos/graph-api.md#use-pydantic-models-for-graph-state) as your graph state if you want recursive data validation (though note that pydantic is less performant than a `TypedDict` or `dataclass`).\\n\\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the [guide here](../how-tos/graph-api.md#define-input-and-output-schemas) for how to use.\\n\\n#### Multiple schemas\\n\\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\\n\\n- Internal nodes can pass information that is not required in the graph\\'s input / output.\\n- We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\\n\\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`.\\n\\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an \"internal\" schema that contains _all_ keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the \"internal\" schema to constrain the input and output of the graph. See [this guide](../how-tos/graph-api.md#define-input-and-output-schemas) for more detail.\\n\\nLet\\'s look at an example:\\n\\n```python\\nclass InputState(TypedDict):\\n    user_input: str\\n\\nclass OutputState(TypedDict):\\n    graph_output: str\\n\\nclass OverallState(TypedDict):\\n    foo: str\\n    user_input: str\\n    graph_output: str\\n\\nclass PrivateState(TypedDict):\\n    bar: str\\n\\ndef node_1(state: InputState) -> OverallState:\\n    # Write to OverallState\\n    return {\"foo\": state[\"user_input\"] + \" name\"}\\n\\ndef node_2(state: OverallState) -> PrivateState:\\n    # Read from OverallState, write to PrivateState\\n    return {\"bar\": state[\"foo\"] + \" is\"}\\n\\ndef node_3(state: PrivateState) -> OutputState:\\n    # Read from PrivateState, write to OutputState\\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\\n\\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\\nbuilder.add_node(\"node_1\", node_1)\\nbuilder.add_node(\"node_2\", node_2)\\nbuilder.add_node(\"node_3\", node_3)\\nbuilder.add_edge(START, \"node_1\")\\nbuilder.add_edge(\"node_1\", \"node_2\")\\nbuilder.add_edge(\"node_2\", \"node_3\")\\nbuilder.add_edge(\"node_3\", END)\\n\\ngraph = builder.compile()\\ngraph.invoke({\"user_input\":\"My\"})\\n# {\\'graph_output\\': \\'My name is Lance\\'}\\n```\\n\\nThere are two subtle and important points to note here:\\n\\n1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node _can write to any state channel in the graph state._ The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.\\n\\n2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because _nodes can also declare additional state channels_ as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.\\n\\n### Reducers\\n\\nReducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\\n\\n#### Default Reducer\\n\\nThese two examples show how to use the default reducer:\\n\\n**Example A:**\\n\\n```python\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: list[str]\\n```\\n\\nIn this example, no reducer functions are specified for any key. Let\\'s assume the input to the graph is:\\n\\n`{\"foo\": 1, \"bar\": [\"hi\"]}`. Let\\'s then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"bye\"]}`\\n\\n**Example B:**\\n\\n```python\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom operator import add\\n\\nclass State(TypedDict):\\n    foo: int\\n    bar: Annotated[list[str], add]\\n```\\n\\nIn this example, we\\'ve used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let\\'s assume the input to the graph is `{\"foo\": 1, \"bar\": [\"hi\"]}`. Let\\'s then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}`. Notice here that the `bar` key is updated by adding the two lists together.\\n\\n### Working with Messages in Graph State\\n\\n#### Why use messages?\\n\\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain\\'s [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as `HumanMessage` (user input) or `AIMessage` (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide.\\n\\n#### Using Messages in your Graph\\n\\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don\\'t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.\\n\\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `add_messages` function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\\n\\n#### Serialization\\n\\nIn addition to keeping track of message IDs, the `add_messages` function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format:\\n\\n```python\\n# this is supported\\n{\"messages\": [HumanMessage(content=\"message\")]}\\n\\n# and this is also supported\\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\\n```\\n\\nSince the state updates are always deserialized into LangChain `Messages` when using `add_messages`, you should use dot notation to access message attributes, like `state[\"messages\"][-1].content`. Below is an example of a graph that uses `add_messages` as its reducer function.\\n\\n```python\\nfrom langchain_core.messages import AnyMessage\\nfrom langgraph.graph.message import add_messages\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\n\\nclass GraphState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n```\\n\\n#### MessagesState\\n\\nSince having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the `add_messages` reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\\n\\n```python\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    documents: list[str]\\n```\\n\\n## Nodes\\n\\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\\n\\n1. `state`: The [state](#state) of the graph\\n2. `config`: A `RunnableConfig` object that contains configuration information like `thread_id` and tracing information like `tags`\\n3. `runtime`: A `Runtime` object that contains [runtime `context`](#runtime-context) and other information like `store` and `stream_writer`\\n\\nSimilar to `NetworkX`, you add these nodes to a graph using the [add_node](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_node) method:\\n\\n```python\\nfrom dataclasses import dataclass\\nfrom typing_extensions import TypedDict\\n\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.runtime import Runtime\\n\\nclass State(TypedDict):\\n    input: str\\n    results: str\\n\\n@dataclass\\nclass Context:\\n    user_id: str\\n\\nbuilder = StateGraph(State)\\n\\ndef plain_node(state: State):\\n    return state\\n\\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\\n    print(\"In node: \", runtime.context.user_id)\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\ndef node_with_config(state: State, config: RunnableConfig):\\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\\n    return {\"results\": f\"Hello, {state[\\'input\\']}!\"}\\n\\nbuilder.add_node(\"plain_node\", plain_node)\\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\\nbuilder.add_node(\"node_with_config\", node_with_config)\\n...\\n```\\n\\nBehind the scenes, functions are converted to [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)s, which add batch and async support to your function, along with native tracing and debugging.\\n\\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\\n\\n```python\\nbuilder.add_node(my_node)\\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\\n```\\n\\n### `START` Node\\n\\nThe `START` Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\\n\\n```python\\nfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n```\\n\\n### `END` Node\\n\\nThe `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\\n\\n```python\\nfrom langgraph.graph import END\\n\\ngraph.add_edge(\"node_a\", END)\\n```\\n\\n### Node Caching\\n\\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\\n\\n- Specify a cache when compiling a graph (or specifying an entrypoint)\\n- Specify a cache policy for nodes. Each cache policy supports:\\n  - `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.\\n  - `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.\\n\\nFor example:\\n\\n```python\\nimport time\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.cache.memory import InMemoryCache\\nfrom langgraph.types import CachePolicy\\n\\nclass State(TypedDict):\\n    x: int\\n    result: int\\n\\nbuilder = StateGraph(State)\\n\\ndef expensive_node(state: State) -> dict[str, int]:\\n    # expensive computation\\n    time.sleep(2)\\n    return {\"result\": state[\"x\"] * 2}\\n\\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\\nbuilder.set_entry_point(\"expensive_node\")\\nbuilder.set_finish_point(\"expensive_node\")\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))  # (1)!\\n[{\\'expensive_node\\': {\\'result\\': 10}}]\\nprint(graph.invoke({\"x\": 5}, stream_mode=\\'updates\\'))  # (2)!\\n[{\\'expensive_node\\': {\\'result\\': 10}, \\'__metadata__\\': {\\'cached\\': True}}]\\n```\\n\\n1. First run takes two seconds to run (due to mocked expensive computation).\\n2. Second run utilizes cache and returns quickly.\\n\\n## Edges\\n\\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\\n\\n- Normal Edges: Go directly from one node to the next.\\n- Conditional Edges: Call a function to determine which node(s) to go to next.\\n- Entry Point: Which node to call first when user input arrives.\\n- Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\\n\\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.\\n\\n### Normal Edges\\n\\nIf you **always** want to go from node A to node B, you can use the [add_edge](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly.\\n\\n```python\\ngraph.add_edge(\"node_a\", \"node_b\")\\n```\\n\\n### Conditional Edges\\n\\nIf you want to **optionally** route to 1 or more edges (or optionally terminate), you can use the [add_conditional_edges](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a \"routing function\" to call after that node is executed:\\n\\n```python\\ngraph.add_conditional_edges(\"node_a\", routing_function)\\n```\\n\\nSimilar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.\\n\\nBy default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\\n\\nYou can optionally provide a dictionary that maps the `routing_function`\\'s output to the name of the next node.\\n\\n```python\\ngraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\\n```\\n\\n!!! tip\\n\\nUse [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.\\n\\n### Entry Point\\n\\nThe entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph.\\n\\n```python\\nfrom langgraph.graph import START\\n\\ngraph.add_edge(START, \"node_a\")\\n```\\n\\n### Conditional Entry Point\\n\\nA conditional entry point lets you start at different nodes depending on custom logic. You can use [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to accomplish this.\\n\\n```python\\nfrom langgraph.graph import START\\n\\ngraph.add_conditional_edges(START, routing_function)\\n```\\n\\nYou can optionally provide a dictionary that maps the `routing_function`\\'s output to the name of the next node.\\n\\n```python\\ngraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\\n```\\n\\n## `Send`\\n\\nBy default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object).\\n\\nTo support this design pattern, LangGraph supports returning [`Send`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node.\\n\\n```python\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\\'subjects\\']]\\n\\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\\n```\\n\\n## `Command`\\n\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [`Command`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions:\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n```\\n\\nWith `Command` you can also achieve dynamic control flow behavior (identical to [conditional edges](#conditional-edges)):\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    if state[\"foo\"] == \"bar\":\\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\\n```\\n\\n!!! important\\n\\nWhen returning `Command` in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal[\"my_other_node\"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`.\\n\\nCheck out this [how-to guide](../how-tos/graph-api.md#combine-control-flow-and-state-updates-with-command) for an end-to-end example of how to use `Command`.\\n\\n### When should I use Command instead of conditional edges?\\n\\n- Use `Command` when you need to **both** update the graph state **and** route to a different node. For example, when implementing [multi-agent handoffs](./multi_agent.md#handoffs) where it\\'s important to route to a different agent and pass some information to that agent.\\n- Use [conditional edges](#conditional-edges) to route between nodes conditionally without updating the state.\\n\\n### Navigating to a node in a parent graph\\n\\nIf you are using [subgraphs](./subgraphs.md), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:\\n\\n```python\\ndef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n```\\n\\n!!! note\\n\\nSetting `graph` to `Command.PARENT` will navigate to the closest parent graph.\\n\\n!!! important \"State updates with `Command.PARENT`\"\\n\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you\\'re updating in the parent graph state. See this [example](../how-tos/graph-api.md#navigate-to-a-node-in-a-parent-graph).\\n\\nThis is particularly useful when implementing [multi-agent handoffs](./multi_agent.md#handoffs).\\n\\nCheck out [this guide](../how-tos/graph-api.md#navigate-to-a-node-in-a-parent-graph) for detail.\\n\\n### Using inside tools\\n\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\\n\\nRefer to [this guide](../how-tos/graph-api.md#use-inside-tools) for detail.\\n\\n### Human-in-the-loop\\n\\n`Command` is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, `Command` is then used to supply the input and resume execution via `Command(resume=\"User input\")`. Check out [this conceptual guide](./human_in_the_loop.md) for more information.\\n\\n## Graph Migrations\\n\\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\\n\\n- For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\\n- For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.\\n- For modifying state, we have full backwards and forwards compatibility for adding and removing keys\\n- State keys that are renamed lose their saved state in existing threads\\n- State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.\\n\\n## Runtime Context\\n\\nWhen creating a graph, you can specify a `context_schema` for runtime context passed to nodes. This is useful for passing\\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\\n\\n```python\\n@dataclass\\nclass ContextSchema:\\n    llm_provider: str = \"openai\"\\n\\ngraph = StateGraph(State, context_schema=ContextSchema)\\n```\\n\\nYou can then pass this context into the graph using the `context` parameter of the `invoke` method.\\n\\n```python\\ngraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\\n```\\n\\nYou can then access and use this context inside a node or conditional edge:\\n\\n```python\\nfrom langgraph.runtime import Runtime\\n\\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\\n    llm = get_llm(runtime.context.llm_provider)\\n    ...\\n```\\n\\nSee [this guide](../how-tos/graph-api.md#add-runtime-configuration) for a full breakdown on configuration.\\n:::\\n\\n### Recursion Limit', source='langgraph-llms-full.txt', metadata={'score': 0.5490933656692505, 'fallback': True, 'confidence': 0.9})",
    "Document(content='The recursion limit sets the maximum number of [super-steps](#graphs) the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `.invoke`/`.stream` via the config dictionary. Importantly, `recursion_limit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below:\\n\\n```python\\ngraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\\n```\\n\\nRead [this how-to](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/) to learn more about how the recursion limit works.\\n\\n## Visualization\\n\\nIt\\'s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](../how-tos/graph-api.md#visualize-your-graph) for more info.\\n\\n---\\nconcepts/langgraph_control_plane.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Control Plane\\n\\nThe term \"control plane\" is used broadly to refer to the control plane UI where users create and update [LangGraph Servers](./langgraph_server.md) (deployments) and the control plane APIs that support the UI experience.\\n\\nWhen a user makes an update through the control plane UI, the update is stored in the control plane state. The [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application polls for these updates by calling the control plane APIs.\\n\\n## Control Plane UI\\n\\nFrom the control plane UI, you can:\\n\\n- View a list of outstanding deployments.\\n- View details of an individual deployment.\\n- Create a new deployment.\\n- Update a deployment.\\n- Update environment variables for a deployment.\\n- View build and server logs of a deployment.\\n- View deployment metrics such as CPU and memory usage.\\n- Delete a deployment.\\n\\nThe Control Plane UI is embedded in [LangSmith](https://docs.smith.langchain.com/langgraph_cloud).\\n\\n## Control Plane API\\n\\nThis section describes the data model of the control plane API. The API is used to create, update, and delete deployments. See the [control plane API reference](../cloud/reference/api/api_ref_control_plane.md) for more details.\\n\\n### Deployment\\n\\nA deployment is an instance of a LangGraph Server. A single deployment can have many revisions.\\n\\n### Revision\\n\\nA revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update secrets for a deployment, a new revision must be created.\\n\\n## Control Plane Features\\n\\nThis section describes various features of the control plane.\\n\\n### Deployment Types\\n\\nFor simplicity, the control plane offers two deployment types with different resource allocations: `Development` and `Production`.\\n\\n| **Deployment Type** | **CPU/Memory**  | **Scaling**       | **Database**                                                                     |\\n| ------------------- | --------------- | ----------------- | -------------------------------------------------------------------------------- |\\n| Development         | 1 CPU, 1 GB RAM | Up to 1 replica   | 10 GB disk, no backups                                                           |\\n| Production          | 2 CPU, 2 GB RAM | Up to 10 replicas | Autoscaling disk, automatic backups, highly available (multi-zone configuration) |\\n\\nCPU and memory resources are per replica.\\n\\n!!! warning \"Immutable Deployment Type\"\\n\\nOnce a deployment is created, the deployment type cannot be changed.\\n\\n!!! info \"Self-Hosted Deployment\"\\nResources for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments can be fully customized. Deployment types are only applicable for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\\n\\n#### Production\\n\\n`Production` type deployments are suitable for \"production\" workloads. For example, select `Production` for customer-facing applications in the critical path.\\n\\nResources for `Production` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact support@langchain.dev to request an increase in resources.\\n\\n#### Development\\n\\n`Development` type deployments are suitable development and testing. For example, select `Development` for internal testing environments. `Development` type deployments are not suitable for \"production\" workloads.\\n\\n!!! danger \"Preemptible Compute Infrastructure\"\\n`Development` type deployments (API server, queue server, and database) are provisioned on preemptible compute infrastructure. This means the compute infrastructure **may be terminated at any time without notice**. This may result in intermittent...\\n\\n- Redis connection timeouts/errors\\n    - Postgres connection timeouts/errors\\n    - Failed or retrying background runs\\n\\nThis behavior is expected. Preemptible compute infrastructure **significantly reduces the cost to provision a `Development` type deployment**. By design, LangGraph Server is fault-tolerant. The implementation will automatically attempt to recover from Redis/Postgres connection errors and retry failed background runs.\\n\\n`Production` type deployments are provisioned on durable compute infrastructure, not preemptible compute infrastructure.\\n\\nDatabase disk size for `Development` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. For most use cases, [TTLs](../how-tos/ttl/configure_ttl.md) should be configured to manage disk usage. Contact support@langchain.dev to request an increase in resources.\\n\\n### Database Provisioning\\n\\nThe control plane and [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application coordinate to automatically create a Postgres database for each deployment. The database serves as the [persistence layer](../concepts/persistence.md) for the deployment.\\n\\nWhen implementing a LangGraph application, a [checkpointer](../concepts/persistence.md#checkpointer-libraries) does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured.\\n\\nThere is no direct access to the database. All access to the database occurs through the [LangGraph Server](../concepts/langgraph_server.md).\\n\\nThe database is never deleted until the deployment itself is deleted.\\n\\n!!! info\\nA custom Postgres instance can be configured for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.\\n\\n### Asynchronous Deployment\\n\\nInfrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.\\n\\n- When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.\\n- When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.\\n- The deployment process for each revision contains a build step, which can take up to a few minutes.\\n\\nThe control plane and [LangGraph Data Plane](./langgraph_data_plane.md) \"listener\" application coordinate to achieve asynchronous deployments.\\n\\n### Monitoring\\n\\nAfter a deployment is ready, the control plane monitors the deployment and records various metrics, such as:\\n\\n- CPU and memory usage of the deployment.\\n- Number of container restarts.\\n- Number of replicas (this will increase with [autoscaling](../concepts/langgraph_data_plane.md#autoscaling)).\\n- [Postgres](../concepts/langgraph_data_plane.md#postgres) CPU, memory usage, and disk usage.\\n- [LangGraph Server queue](../concepts/langgraph_server.md#persistence-and-task-queue) pending/active run count.\\n- [LangGraph Server API](../concepts/langgraph_server.md) success response count, error response count, and latency.\\n\\nThese metrics are displayed as charts in the Control Plane UI.\\n\\n### LangSmith Integration\\n\\nA [LangSmith](https://docs.smith.langchain.com/) tracing project and LangSmith API key are automatically created for each deployment. The deployment uses the API key to automatically send traces to LangSmith.\\n\\n- The tracing project has the same name as the deployment.\\n- The API key has the description `LangGraph Platform: <deployment_name>`.\\n- The API key is never revealed and cannot be deleted manually.\\n- When creating a deployment, the `LANGCHAIN_TRACING` and `LANGSMITH_API_KEY`/`LANGCHAIN_API_KEY` environment variables do not need to be specified; they are set automatically by the control plane.\\n\\nWhen a deployment is deleted, the traces and the tracing project are not deleted. However, the API will be deleted when the deployment is deleted.\\n\\n---\\nconcepts/durable_execution.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# Durable Execution\\n\\n**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](./human_in_the_loop.md), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).\\n\\nLangGraph\\'s built-in [persistence](./persistence.md) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](./human_in_the_loop.md) interactions -- it can be resumed from its last recorded state.\\n\\n!!! tip\\n\\nIf you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\\n    To make the most of durable execution, ensure that your workflow is designed to be [deterministic](#determinism-and-consistent-replay) and [idempotent](#determinism-and-consistent-replay) and wrap any side effects or non-deterministic operations inside [tasks](./functional_api.md#task). You can use [tasks](./functional_api.md#task) from both the [StateGraph (Graph API)](./low_level.md) and the [Functional API](./functional_api.md).\\n\\n## Requirements\\n\\nTo leverage durable execution in LangGraph, you need to:\\n\\n1. Enable [persistence](./persistence.md) in your workflow by specifying a [checkpointer](./persistence.md#checkpointer-libraries) that will save workflow progress.\\n2. Specify a [thread identifier](./persistence.md#threads) when executing a workflow. This will track the execution history for a particular instance of the workflow.\\n\\n3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside [tasks](https://langchain-ai.github.io/langgraph/reference/func/#langgraph.func.task) to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see [Determinism and Consistent Replay](#determinism-and-consistent-replay).\\n\\n## Determinism and Consistent Replay\\n\\nWhen you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate [starting point](#starting-points-for-resuming-workflows) from which to pick up where it left off. This means that the workflow will replay all steps from the [starting point](#starting-points-for-resuming-workflows) until it reaches the point where it was stopped.\\n\\nAs a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside [tasks](./functional_api.md#task) or [nodes](./low_level.md#nodes).\\n\\nTo ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:\\n\\n- **Avoid Repeating Work**: If a [node](./low_level.md#nodes) contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.\\n- **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.\\n- **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow\\'s resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.\\n\\nFor some examples of pitfalls to avoid, see the [Common Pitfalls](./functional_api.md#common-pitfalls) section in the functional API, which shows\\nhow to structure your code using **tasks** to avoid these issues. The same principles apply to the [StateGraph (Graph API)](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph).\\n\\n## Durability modes\\n\\nLangGraph supports three durability modes that allow you to balance performance and data consistency based on your application\\'s requirements. The durability modes, from least to most durable, are as follows:\\n\\n- [`\"exit\"`](#exit)\\n- [`\"async\"`](#async)\\n- [`\"sync\"`](#sync)\\n\\nA higher durability mode add more overhead to the workflow execution.\\n\\n!!! version-added \"Added in version 0.6.0\"\\n\\nUse the `durability` parameter instead of `checkpoint_during` (deprecated in v0.6.0) for persistence policy management:\\n    \\n    * `durability=\"async\"` replaces `checkpoint_during=True`\\n    * `durability=\"exit\"` replaces `checkpoint_during=False`\\n    \\n    for persistence policy management, with the following mapping:\\n\\n* `checkpoint_during=True` -> `durability=\"async\"`\\n    * `checkpoint_during=False` -> `durability=\"exit\"`\\n\\n### `\"exit\"`\\n\\nChanges are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.\\n\\n### `\"async\"`\\n\\nChanges are persisted asynchronously while the next step executes. This provides good performance and durability, but there\\'s a small risk that checkpoints might not be written if the process crashes during execution.\\n\\n### `\"sync\"`\\n\\nChanges are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\\n\\nYou can specify the durability mode when calling any graph execution method:\\n\\n```python\\ngraph.stream(\\n    {\"input\": \"test\"}, \\n    durability=\"sync\"\\n)\\n```\\n\\n## Using tasks in nodes\\n\\nIf a [node](./low_level.md#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.\\n\\n=== \"Original\"\\n\\n```python hl_lines=\"16\"\\n    from typing import NotRequired\\n    from typing_extensions import TypedDict\\n    import uuid\\n\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n    from langgraph.graph import StateGraph, START, END\\n    import requests\\n\\n# Define a TypedDict to represent the state\\n    class State(TypedDict):\\n        url: str\\n        result: NotRequired[str]\\n\\ndef call_api(state: State):\\n        \"\"\"Example node that makes an API request.\"\"\"\\n        result = requests.get(state[\\'url\\']).text[:100]  # Side-effect\\n        return {\\n            \"result\": result\\n        }\\n\\n# Create a StateGraph builder and add a node for the call_api function\\n    builder = StateGraph(State)\\n    builder.add_node(\"call_api\", call_api)\\n\\n# Connect the start and end nodes to the call_api node\\n    builder.add_edge(START, \"call_api\")\\n    builder.add_edge(\"call_api\", END)\\n\\n# Specify a checkpointer\\n    checkpointer = InMemorySaver()\\n\\n# Compile the graph with the checkpointer\\n    graph = builder.compile(checkpointer=checkpointer)\\n\\n# Define a config with a thread ID.\\n    thread_id = uuid.uuid4()\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n# Invoke the graph\\n    graph.invoke({\"url\": \"https://www.example.com\"}, config)\\n    ```\\n\\n=== \"With task\"\\n\\n```python hl_lines=\"19 23\"\\n    from typing import NotRequired\\n    from typing_extensions import TypedDict\\n    import uuid\\n\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n    from langgraph.func import task\\n    from langgraph.graph import StateGraph, START, END\\n    import requests\\n\\n# Define a TypedDict to represent the state\\n    class State(TypedDict):\\n        urls: list[str]\\n        result: NotRequired[list[str]]\\n\\n@task\\n    def _make_request(url: str):\\n        \"\"\"Make a request.\"\"\"\\n        return requests.get(url).text[:100]\\n\\ndef call_api(state: State):\\n        \"\"\"Example node that makes an API request.\"\"\"\\n        requests = [_make_request(url) for url in state[\\'urls\\']]\\n        results = [request.result() for request in requests]\\n        return {\\n            \"results\": results\\n        }\\n\\n# Create a StateGraph builder and add a node for the call_api function\\n    builder = StateGraph(State)\\n    builder.add_node(\"call_api\", call_api)\\n\\n# Connect the start and end nodes to the call_api node\\n    builder.add_edge(START, \"call_api\")\\n    builder.add_edge(\"call_api\", END)\\n\\n# Specify a checkpointer\\n    checkpointer = InMemorySaver()\\n\\n# Compile the graph with the checkpointer\\n    graph = builder.compile(checkpointer=checkpointer)\\n\\n# Define a config with a thread ID.\\n    thread_id = uuid.uuid4()\\n    config = {\"configurable\": {\"thread_id\": thread_id}}\\n\\n# Invoke the graph\\n    graph.invoke({\"urls\": [\"https://www.example.com\"]}, config)\\n    ```\\n\\n## Resuming Workflows\\n\\nOnce you have enabled durable execution in your workflow, you can resume execution for the following scenarios:\\n\\n- **Pausing and Resuming Workflows:** Use the [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Interrupt) function to pause a workflow at specific points and the [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) primitive to resume it with updated state. See [**Human-in-the-Loop**](./human_in_the_loop.md) for more details.\\n- **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `None` as the input value (see this [example](../how-tos/use-functional-api.md#resuming-after-an-error) with the functional API).\\n\\n## Starting Points for Resuming Workflows\\n\\n- If you\\'re using a [StateGraph (Graph API)](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph), the starting point is the beginning of the [**node**](./low_level.md#nodes) where execution stopped.\\n- If you\\'re making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.\\n  Inside the subgraph, the starting point will be the specific [**node**](./low_level.md#nodes) where execution stopped.\\n- If you\\'re using the Functional API, the starting point is the beginning of the [**entrypoint**](./functional_api.md#entrypoint) where execution stopped.\\n\\n---\\nconcepts/langgraph_data_plane.md\\n---\\n\\n---\\nsearch:\\n  boost: 2\\n---\\n\\n# LangGraph Data Plane\\n\\nThe term \"data plane\" is used broadly to refer to [LangGraph Servers](./langgraph_server.md) (deployments), the corresponding infrastructure for each server, and the \"listener\" application that continuously polls for updates from the [LangGraph Control Plane](./langgraph_control_plane.md).\\n\\n## Server Infrastructure\\n\\nIn addition to the [LangGraph Server](./langgraph_server.md) itself, the following infrastructure components for each server are also included in the broad definition of \"data plane\":\\n\\n- Postgres\\n- Redis\\n- Secrets store\\n- Autoscalers\\n\\n## \"Listener\" Application\\n\\nThe data plane \"listener\" application periodically calls [control plane APIs](../concepts/langgraph_control_plane.md#control-plane-api) to:\\n\\n- Determine if new deployments should be created.\\n- Determine if existing deployments should be updated (i.e. new revisions).\\n- Determine if existing deployments should be deleted.\\n\\nIn other words, the data plane \"listener\" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.\\n\\n## Postgres\\n\\nPostgres is the persistence layer for all user, run, and long-term memory data in a LangGraph Server. This stores both checkpoints (see more info [here](./persistence.md)), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info [here](./persistence.md#memory-store)).\\n\\n## Redis\\n\\nRedis is used in each LangGraph Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.\\n\\n### Communication\\n\\nAll runs in a LangGraph Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.\\n\\n1. A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from Postgres by the worker.\\n2. A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.\\n3. A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open `/stream` request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.\\n\\n### Ephemeral metadata\\n\\nRuns in a LangGraph Server may be retried for specific failures (currently only for transient Postgres errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.\\n\\n## Data Plane Features\\n\\nThis section describes various features of the data plane.\\n\\n### Data Region\\n\\n!!! info \"Only for Cloud SaaS\"\\n    Data regions are only applicable for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\\n\\nDeployments can be created in 2 data regions: US and EU\\n\\nThe data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.\\n\\n### Autoscaling\\n\\n[`Production` type](../concepts/langgraph_control_plane.md#deployment-types) deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:\\n\\n1. CPU utilization\\n1. Memory utilization\\n1. Number of pending (in progress) [runs](./assistants.md#execution)\\n\\nFor CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.\\n\\nFor number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs in 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).\\n\\nEach metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the largest number of containers.\\n\\nScale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This \"cool down\" period ensures that deployments do not scale up and down too frequently.\\n\\n### Static IP Addresses\\n\\n!!! info \"Only for Cloud SaaS\"\\nStatic IP addresses are only available for [Cloud SaaS](../concepts/langgraph_cloud.md) deployments.\\n\\nAll traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:\\n\\n| US             | EU             |\\n| -------------- | -------------- |\\n| 35.197.29.146  | 34.13.192.67   |\\n| 34.145.102.123 | 34.147.105.64  |\\n| 34.169.45.153  | 34.90.22.166   |\\n| 34.82.222.17   | 34.147.36.213  |\\n| 35.227.171.135 | 34.32.137.113  |\\n| 34.169.88.30   | 34.91.238.184  |\\n| 34.19.93.202   | 35.204.101.241 |\\n| 34.19.34.50    | 35.204.48.32   |\\n\\n### Custom Postgres\\n\\n!!! info\\nCustom Postgres instances are only available for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_data_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.\\n\\nA custom Postgres instance can be used instead of the [one automatically created by the control plane](./langgraph_control_plane.md#database-provisioning). Specify the [`POSTGRES_URI_CUSTOM`](../cloud/reference/env_var.md#postgres_uri_custom) environment variable to use a custom Postgres instance.\\n\\nMultiple deployments can share the same Postgres instance. For example, for `Deployment A`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`. `<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.\\n\\n### Custom Redis\\n\\n!!! info\\nCustom Redis instances are only available for [Self-Hosted Data Plane](../concepts/langgraph_self_hosted_control_plane.md) and [Self-Hosted Control Plane](../concepts/langgraph_self_hosted_control_plane.md) deployments.', source='langgraph-llms-full.txt', metadata={'score': 0.636741578578949, 'fallback': True, 'confidence': 0.9})"
  ],
  "final_answer": "Okay, here's how to handle errors and retries in LangGraph nodes, focusing on LangGraph concepts:\n\n**1. Conditional Node for Exception Handling**\n\nYou can use a `ConditionalNode` to route execution based on whether a node raises an exception.  The key is to wrap the potentially failing node's logic in a `try...except` block and then return a specific value that the conditional node can use to route to an error handling path.\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict, Literal\nfrom typing import Dict, Any\n\nclass State(TypedDict):\n    task: str\n    attempts: int\n    result: str\n\ndef task_node(state: State) -> Dict[str, Any]:\n    try:\n        # Simulate a task that might fail\n        if state[\"task\"] == \"risky_task\" and state[\"attempts\"] < 2:\n            raise ValueError(\"Task failed\")\n        return {\"result\": f\"Task {state['task']} successful\"}\n    except Exception as e:\n        return {\"result\": str(e)}\n\ndef check_result(state: State) -> Literal[\"success\", \"retry\", \"fail\"]:\n    if \"Task successful\" in state[\"result\"]:\n        return \"success\"\n    elif state[\"attempts\"] < 3:\n        return \"retry\"\n    else:\n        return \"fail\"\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"task_node\", task_node)\nbuilder.add_node(\"check_result\", check_result)\n\nbuilder.add_conditional_edges(\n    \"check_result\",\n    check_result,\n    {\n        \"success\": END,\n        \"retry\": \"task_node\",\n        \"fail\": END,\n    },\n)\n\nbuilder.add_edge(START, \"task_node\")\nbuilder.add_edge(\"task_node\", \"check_result\")\n\ngraph = builder.compile()\n\n# Example usage\nresult = graph.invoke({\"task\": \"risky_task\", \"attempts\": 0, \"result\": \"\"})\nprint(result)\n```\n\n**2. Retry Mechanism within a Node**\n\nImplement a retry mechanism within a node's function using Python's `try...except` block.  The node function should catch the exception, update the state (e.g., incrementing a retry counter), and return a signal to the graph indicating that a retry is needed.\n\n**3. Looping Back for Retries**\n\nUse `StateGraph.add_conditional_edges()` to create a loop back to the same node if an error occurs. The conditional edge's routing function will determine whether to retry based on the error and retry count.\n\n**4. Tracking Retry Count in Global State**\n\nMaintain a global state within the LangGraph to track the number of retries for a specific node. Use a `ConditionalNode` to exit the retry loop after a maximum number of attempts.\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict, Literal\nfrom typing import Dict, Any\n\nclass State(TypedDict):\n    task: str\n    attempts: int\n    result: str\n\ndef task_node(state: State) -> Dict[str, Any]:\n    try:\n        # Simulate a task that might fail\n        if state[\"task\"] == \"risky_task\" and state[\"attempts\"] < 2:\n            raise ValueError(\"Task failed\")\n        return {\"result\": f\"Task {state['task']} successful\"}\n    except Exception as e:\n        return {\"result\": str(e)}\n\ndef check_result(state: State) -> Literal[\"success\", \"retry\", \"fail\"]:\n    if \"Task successful\" in state[\"result\"]:\n        return \"success\"\n    elif state[\"attempts\"] < 3:\n        return \"retry\"\n    else:\n        return \"fail\"\n\ndef increment_attempts(state: State) -> Dict[str, Any]:\n    return {\"attempts\": state[\"attempts\"] + 1}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"task_node\", task_node)\nbuilder.add_node(\"check_result\", check_result)\nbuilder.add_node(\"increment_attempts\", increment_attempts)\n\nbuilder.add_conditional_edges(\n    \"check_result\",\n    check_result,\n    {\n        \"success\": END,\n        \"retry\": \"increment_attempts\",\n        \"fail\": END,\n    },\n)\n\nbuilder.add_edge(\"increment_attempts\", \"task_node\")\nbuilder.add_edge(START, \"task_node\")\nbuilder.add_edge(\"task_node\", \"check_result\")\n\ngraph = builder.compile()\n\n# Example usage\nresult = graph.invoke({\"task\": \"risky_task\", \"attempts\": 0, \"result\": \"\"})\nprint(result)\n```\n\n**5. Terminating Graph Execution with `END`**\n\nIf a node consistently fails after multiple retries, use the `END` keyword to terminate the graph execution gracefully.  The conditional edge's routing function should direct execution to the `END` node when the maximum retry count is reached.\n\n**Important Considerations:**\n\n*   **Checkpointers:** To persist the retry count and other state information across retries, use a checkpointer (e.g., `MemorySaver`, `SqliteSaver`, `PostgresSaver`).  Configure the checkpointer when compiling the graph.\n*   **Thread Identifiers:**  When using checkpointers, ensure you provide a unique `thread_id` in the `config` dictionary when invoking the graph. This allows LangGraph to resume the correct execution thread.\n*   **Idempotency:** Ensure that the operations within your nodes are idempotent, meaning that retrying them multiple times has the same effect as running them once. This is crucial to avoid unintended side effects.  If operations are *not* idempotent, wrap them in `task` functions.\n*   **Error Handling:**  Implement robust error handling within your node functions to catch specific exceptions and determine whether a retry is appropriate.\n*   **State Management:** Carefully manage the state of your graph to ensure that the retry count and other relevant information are updated correctly.\n*   **Durability:**  Consider the durability mode (`\"exit\"`, `\"async\"`, `\"sync\"`) to balance performance and data consistency.\n\n```python\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict, Literal\nfrom typing import Dict, Any\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nimport os\n\nclass State(TypedDict):\n    task: str\n    attempts: int\n    result: str\n\ndef task_node(state: State) -> Dict[str, Any]:\n    try:\n        # Simulate a task that might fail\n        if state[\"task\"] == \"risky_task\" and state[\"attempts\"] < 2:\n            raise ValueError(\"Task failed\")\n        return {\"result\": f\"Task {state['task']} successful\"}\n    except Exception as e:\n        return {\"result\": str(e)}\n\ndef check_result(state: State) -> Literal[\"success\", \"retry\", \"fail\"]:\n    if \"Task successful\" in state[\"result\"]:\n        return \"success\"\n    elif state[\"attempts\"] < 3:\n        return \"retry\"\n    else:\n        return \"fail\"\n\ndef increment_attempts(state: State) -> Dict[str, Any]:\n    return {\"attempts\": state[\"attempts\"] + 1}\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"task_node\", task_node)\nbuilder.add_node(\"check_result\", check_result)\nbuilder.add_node(\"increment_attempts\", increment_attempts)\n\nbuilder.add_conditional_edges(\n    \"check_result\",\n    check_result,\n    {\n        \"success\": END,\n        \"retry\": \"increment_attempts\",\n        \"fail\": END,\n    },\n)\n\nbuilder.add_edge(\"increment_attempts\", \"task_node\")\nbuilder.add_edge(START, \"task_node\")\nbuilder.add_edge(\"task_node\", \"check_result\")\n\n# Checkpointer setup\ndb_file = os.path.join(\"./\", \"graph.db\")\nif os.path.exists(db_file):\n    os.remove(db_file)\n\ncheckpointer = SqliteSaver(database_path=db_file)\n\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Example usage\nconfig = {\"configurable\": {\"thread_id\": \"my_thread\"}}\nresult = graph.invoke({\"task\": \"risky_task\", \"attempts\": 0, \"result\": \"\"}, config)\nprint(result)\n```\n\nThis comprehensive approach allows you to build robust and resilient LangGraph workflows that can handle errors gracefully and recover from failures.",
  "reasoning_style": "hybrid",
  "orc_plan": "Okay, here's a breakdown of how to handle errors and retries in LangGraph nodes, focusing on LangGraph concepts and avoiding memory modules:\n\n1.  How can I use the `ConditionalNode` to route execution based on whether a node raises an exception?\n2.  How can I implement a retry mechanism within a node's function using Python's `try...except` block and then signal a retry condition to the graph?\n3.  How can I use the `StateGraph.add_edge()` method to create a loop back to the same node if an error occurs, effectively implementing a retry?\n4.  How can I use a global state within the LangGraph to track the number of retries for a specific node and use a `ConditionalNode` to exit the retry loop after a maximum number of attempts?\n5.  How can I use the `END` keyword to terminate the graph execution gracefully if a node consistently fails after multiple retries?",
  "orc_subquestions": [
    "How can I use the `ConditionalNode` to route execution based on whether a node raises an exception?",
    "How can I implement a retry mechanism within a node's function using Python's `try...except` block and then signal a retry condition to the graph?",
    "How can I use the `StateGraph.add_edge()` method to create a loop back to the same node if an error occurs, effectively implementing a retry?",
    "How can I use a global state within the LangGraph to track the number of retries for a specific node and use a `ConditionalNode` to exit the retry loop after a maximum number of attempts?",
    "How can I use the `END` keyword to terminate the graph execution gracefully if a node consistently fails after multiple retries?"
  ]
}